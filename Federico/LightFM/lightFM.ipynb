{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "while os.path.split(os.getcwd())[1] != 'RecSysChallenge2023-Team':\n",
    "    os.chdir('..')\n",
    "sys.path.insert(1, os.getcwd())\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_user_wise\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "import Daniele.Utils.MyDataManager as dm\n",
    "import Daniele.Utils.MatrixManipulation as mm\n",
    "\n",
    "URMv = dm.getURMviews()\n",
    "URMo = dm.getURMopen()\n",
    "#URM_all = mm.defaultExplicitURM(urmv=URMv, urmo=URMo, normalize=False, add_aug=False)\n",
    "URM_all = URMo + URMv\n",
    "URM_all.data = np.ones(len(URM_all.data)) #Binaria\n",
    "\n",
    "ICMt=dm.getICMt()\n",
    "ICMl=dm.getICMl()\n",
    "\n",
    "URM_train_validation, URM_test = split_train_in_two_percentage_user_wise(URM_all, train_percentage = 0.85)\n",
    "# URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train_validation, train_percentage = 0.80)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<lightfm.lightfm.LightFM at 0x7faf5db8da30>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "\n",
    "model = LightFM(\n",
    "    loss='warp',\n",
    "    #item_alpha=0.1,\n",
    "    #user_alpha=0.1,\n",
    ")\n",
    "%time\n",
    "model.fit(URM_train_validation, epochs=1, verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train precision: 0.11\n",
      "Test precision: 0.02\n"
     ]
    }
   ],
   "source": [
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "print(\"Train precision: %.2f\" % precision_at_k(model, URM_train_validation, k=10).mean())\n",
    "print(\"Test precision: %.2f\" % precision_at_k(model, URM_test, k=10).mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Ignoring 318 ( 0.8%) Users that have less than 1 test interactions\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LightFM' object has no attribute 'get_URM_train'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m evaluator_test \u001B[38;5;241m=\u001B[39m EvaluatorHoldout(URM_test, [\u001B[38;5;241m10\u001B[39m])\n\u001B[0;32m----> 2\u001B[0m results, result_string \u001B[38;5;241m=\u001B[39m \u001B[43mevaluator_test\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluateRecommender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documenti/Polimi/RecSys/repos/RecSysChallenge2023-Team/Evaluation/Evaluator.py:276\u001B[0m, in \u001B[0;36mEvaluator.evaluateRecommender\u001B[0;34m(self, recommender_object)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time_print \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_users_evaluated \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 276\u001B[0m results_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_evaluation_on_selected_users\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecommender_object\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43musers_to_evaluate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_users_evaluated \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cutoff \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcutoff_list:\n",
      "File \u001B[0;32m~/Documenti/Polimi/RecSys/repos/RecSysChallenge2023-Team/Evaluation/Evaluator.py:453\u001B[0m, in \u001B[0;36mEvaluatorHoldout._run_evaluation_on_selected_users\u001B[0;34m(self, recommender_object, users_to_evaluate, block_size)\u001B[0m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m block_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;66;03m# Reduce block size if estimated memory requirement exceeds 4 GB\u001B[39;00m\n\u001B[1;32m    448\u001B[0m     block_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m([\u001B[38;5;241m1000\u001B[39m, \u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m4\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m1e9\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m8\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m64\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_items), \u001B[38;5;28mlen\u001B[39m(users_to_evaluate)])\n\u001B[1;32m    451\u001B[0m results_dict \u001B[38;5;241m=\u001B[39m _create_empty_metrics_dict(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcutoff_list,\n\u001B[1;32m    452\u001B[0m                                           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_items, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_users,\n\u001B[0;32m--> 453\u001B[0m                                           \u001B[43mrecommender_object\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_URM_train\u001B[49m(),\n\u001B[1;32m    454\u001B[0m                                           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mURM_test,\n\u001B[1;32m    455\u001B[0m                                           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_items_ID,\n\u001B[1;32m    456\u001B[0m                                           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_users_ID,\n\u001B[1;32m    457\u001B[0m                                           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiversity_object)\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_items_flag:\n\u001B[1;32m    461\u001B[0m     recommender_object\u001B[38;5;241m.\u001B[39mset_items_to_ignore(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_items_ID)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LightFM' object has no attribute 'get_URM_train'"
     ]
    }
   ],
   "source": [
    "evaluator_test = EvaluatorHoldout(URM_test, [10])\n",
    "results, result_string = evaluator_test.evaluateRecommender(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
