{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple XGBoost tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the trivial **Iris Dataset** which you can get from directly from sklearn or from the UCI repository.\n",
    "\n",
    "First of all, let's import what is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Iris dataset and split in train and test following the thumb rule of 80/20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **DMatrix** as data structures for both train and test. \n",
    "\n",
    "DMatrix is a internal data structure that used by XGBoost which is optimized for both memory efficiency and training speed.\n",
    "You can construct DMatrix from numpy.arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 2, 1, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set XGBoost parameters:\n",
    "\n",
    "merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # step for each iteration\n",
    "    'silent': 1, # keep it quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 3, # the number of classes \n",
    "    'eval_metric': 'merror'} # evaluation metric \n",
    "\n",
    "num_round = 20  # the number of training iterations (number of trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model and fit it with params:\n",
    "\n",
    "(We could have used also XGBClassifier, which is a scikit-learn compatible class. It is basically just a wrapper over the xgb.train function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:05:54] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-merror:0.01667\n",
      "[2]\ttrain-merror:0.01667\n",
      "[4]\ttrain-merror:0.01667\n",
      "[6]\ttrain-merror:0.01667\n",
      "[8]\ttrain-merror:0.01667\n",
      "[10]\ttrain-merror:0.01667\n",
      "[12]\ttrain-merror:0.01667\n",
      "[14]\ttrain-merror:0.01667\n",
      "[16]\ttrain-merror:0.01667\n",
      "[18]\ttrain-merror:0.00833\n",
      "[19]\ttrain-merror:0.00833\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(params,\n",
    "                  dtrain,\n",
    "                  num_round,\n",
    "                  verbose_eval=2,\n",
    "                  evals=[(dtrain, 'train')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.95 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "preds = model.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print(\"Precision: {:.2f} %\".format(precision_score(y_test, best_preds, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show with an histogram the importance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gain:** it implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Gain'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(model, importance_type='gain', title='Gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cover:** The Cover metric means the relative number of observations related to this feature. Therefore is the number of observations for which the leaf node is determined by the feature. This number will be calculated for all the features in the dataset and the cover will be expressed as a percentage over all features' cover metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Cover'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(model, importance_type='cover', title='Cover')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight (Frequence):** is the percentage representing the relative number of times a particular feature occurs in the trees of the model. Is the number of splits in which the particular feature occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Weight (Frequence)'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(model, importance_type='weight', title='Weight (Frequence)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid overfitting, we can use the early stopping technique looking at the error on our local validation set. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=3)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:05:56] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[0]\ttrain-merror:0.02083\tvalidation-merror:0.00000\n",
      "[2]\ttrain-merror:0.02083\tvalidation-merror:0.00000\n",
      "[4]\ttrain-merror:0.02083\tvalidation-merror:0.00000\n",
      "[6]\ttrain-merror:0.01042\tvalidation-merror:0.04167\n",
      "[8]\ttrain-merror:0.01042\tvalidation-merror:0.04167\n",
      "[10]\ttrain-merror:0.00000\tvalidation-merror:0.08333\n",
      "[12]\ttrain-merror:0.00000\tvalidation-merror:0.04167\n",
      "[14]\ttrain-merror:0.00000\tvalidation-merror:0.08333\n",
      "[16]\ttrain-merror:0.00000\tvalidation-merror:0.08333\n",
      "[18]\ttrain-merror:0.00000\tvalidation-merror:0.08333\n",
      "[19]\ttrain-merror:0.00000\tvalidation-merror:0.08333\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(params,\n",
    "                  dtrain,\n",
    "                  num_round,\n",
    "                  verbose_eval=2,\n",
    "                  evals=[(dtrain, 'train'), (dval, 'validation')],\n",
    "                  early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we plot the precision and feature weight we may see changes in the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.95 %\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(dtest)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print(\"Precision: {:.2f} %\".format(precision_score(y_test, best_preds, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Weight (Frequence)'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(model, importance_type='weight', title='Weight (Frequence)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use XGBoost in RecSys?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint: reranking and hybridization\n",
    "\n",
    "### Steps:\n",
    "* Run your best algorithm and select a number of recommendations higher than the target cutoff, for example if you have to compute MAP@10, get 20 recommendations\n",
    "* Build a dataframe whose samples are the user-item recommendations\n",
    "* Add for each interaction some content features: item features, user features\n",
    "* Add for each interaction some features derived by other algorithms: CBF prediction, hybrid prediction\n",
    "* Add for each interaction other miscellaneous information: profile length, item popularity .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10M: Verifying data consistency...\n",
      "Movielens10M: Verifying data consistency... Passed!\n",
      "DataReader: current dataset is: Movielens10M\n",
      "\tNumber of items: 10681\n",
      "\tNumber of users: 69878\n",
      "\tNumber of interactions in URM_all: 10000054\n",
      "\tValue range in URM_all: 0.50-5.00\n",
      "\tInteraction density: 1.34E-02\n",
      "\tInteractions per user:\n",
      "\t\t Min: 2.00E+01\n",
      "\t\t Avg: 1.43E+02\n",
      "\t\t Max: 7.36E+03\n",
      "\tInteractions per item:\n",
      "\t\t Min: 0.00E+00\n",
      "\t\t Avg: 9.36E+02\n",
      "\t\t Max: 3.49E+04\n",
      "\tGini Index: 0.57\n",
      "\n",
      "\tICM name: ICM_all, Value range: 1.00 / 69.00, Num features: 10126, feature occurrences: 128384, density 1.19E-03\n",
      "\tICM name: ICM_genres, Value range: 1.00 / 1.00, Num features: 20, feature occurrences: 21564, density 1.01E-01\n",
      "\tICM name: ICM_tags, Value range: 1.00 / 69.00, Num features: 10106, feature occurrences: 106820, density 9.90E-04\n",
      "\tICM name: ICM_year, Value range: 6.00E+00 / 2.01E+03, Num features: 1, feature occurrences: 10681, density 1.00E+00\n",
      "\n",
      "\n",
      "Warning: 76 (0.11 %) of 69878 users have no sampled items\n",
      "Warning: 214 (0.31 %) of 69878 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens10MReader import Movielens10MReader\n",
    "\n",
    "data_reader = Movielens10MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.8)\n",
    "\n",
    "ICM_genres = data_loaded.get_loaded_ICM_dict()[\"ICM_genres\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's select the recommendations to use to train XGBoost\n",
    "\n",
    "Several options are possible:\n",
    "- Take the best recommender and select, say, the top-100 recommended items\n",
    "- Take multiple recommenders and use the union of their recommendations\n",
    "- ...\n",
    "\n",
    "In this example a single model will be used to select the candidate user-item interactions to use for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferra\\anaconda3\\envs\\RecSysFramework38_GPU\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemKNNCFRecommender: URM Detected 71 ( 0.7%) items with no interactions.\n",
      "Similarity column 10681 (100.0%), 1055.50 column/sec. Elapsed time 10.12 sec\n"
     ]
    }
   ],
   "source": [
    "from Recommenders.Recommender_import_list import *\n",
    "\n",
    "candidate_generator_recommender = ItemKNNCFRecommender(URM_train)\n",
    "candidate_generator_recommender.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataframe with the predictions\n",
    "\n",
    "Creating dataframes can be quite computationally expensive. Never use \"append\" or iteratively concatenate new elements when you have to run a significant number of iterations, it will take forever. Initialize instead the dimension you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "from xgboost import XGBRanker\n",
    "\n",
    "n_users, n_items = URM_train.shape\n",
    "\n",
    "training_dataframe = pd.DataFrame(index=range(0,n_users), columns = [\"ItemID\"])\n",
    "training_dataframe.index.name='UserID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69873</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69874</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69875</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69876</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID\n",
       "UserID       \n",
       "0         NaN\n",
       "1         NaN\n",
       "2         NaN\n",
       "3         NaN\n",
       "4         NaN\n",
       "...       ...\n",
       "69873     NaN\n",
       "69874     NaN\n",
       "69875     NaN\n",
       "69876     NaN\n",
       "69877     NaN\n",
       "\n",
       "[69878 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 69878/69878 [00:47<00:00, 1463.08it/s]\n"
     ]
    }
   ],
   "source": [
    "cutoff = 30\n",
    "\n",
    "for user_id in tqdm(range(n_users)):    \n",
    "    recommendations = candidate_generator_recommender.recommend(user_id, cutoff = cutoff)\n",
    "    training_dataframe.loc[user_id, \"ItemID\"] = recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[94, 11, 175, 91, 85, 93, 74, 84, 95, 82, 26, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1302, 31, 1429, 1283, 1304, 1300, 108, 37, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[60, 798, 1882, 198, 296, 279, 821, 793, 284, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[175, 74, 75, 9, 77, 4, 1009, 84, 92, 1418, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1073, 176, 1122, 166, 165, 44, 219, 218, 144,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69873</th>\n",
       "      <td>[64, 968, 793, 864, 982, 826, 821, 1234, 62, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69874</th>\n",
       "      <td>[466, 461, 213, 1008, 468, 259, 1093, 1147, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69875</th>\n",
       "      <td>[148, 218, 44, 164, 170, 403, 34, 1656, 146, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69876</th>\n",
       "      <td>[74, 175, 9, 83, 7, 85, 3, 1009, 82, 75, 139, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>[37, 177, 195, 196, 228, 382, 235, 415, 404, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69878 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ItemID\n",
       "UserID                                                   \n",
       "0       [94, 11, 175, 91, 85, 93, 74, 84, 95, 82, 26, ...\n",
       "1       [1302, 31, 1429, 1283, 1304, 1300, 108, 37, 10...\n",
       "2       [60, 798, 1882, 198, 296, 279, 821, 793, 284, ...\n",
       "3       [175, 74, 75, 9, 77, 4, 1009, 84, 92, 1418, 1,...\n",
       "4       [1073, 176, 1122, 166, 165, 44, 219, 218, 144,...\n",
       "...                                                   ...\n",
       "69873   [64, 968, 793, 864, 982, 826, 821, 1234, 62, 8...\n",
       "69874   [466, 461, 213, 1008, 468, 259, 1093, 1147, 19...\n",
       "69875   [148, 218, 44, 164, 170, 403, 34, 1656, 146, 4...\n",
       "69876   [74, 175, 9, 83, 7, 85, 3, 1009, 82, 75, 139, ...\n",
       "69877   [37, 177, 195, 196, 228, 382, 235, 415, 404, 1...\n",
       "\n",
       "[69878 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's transform the dataframe so that we have one recommendation per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>1429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID\n",
       "UserID       \n",
       "0          94\n",
       "0          11\n",
       "0         175\n",
       "0          91\n",
       "0          85\n",
       "...       ...\n",
       "69877    1283\n",
       "69877    1429\n",
       "69877     137\n",
       "69877    1079\n",
       "69877     411\n",
       "\n",
       "[2096340 rows x 1 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.explode(\"ItemID\")\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should add the target column, which is the correct recommendation as contained in the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600004</th>\n",
       "      <td>69877</td>\n",
       "      <td>1646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600005</th>\n",
       "      <td>69877</td>\n",
       "      <td>1660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600006</th>\n",
       "      <td>69877</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600007</th>\n",
       "      <td>69877</td>\n",
       "      <td>2941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600008</th>\n",
       "      <td>69877</td>\n",
       "      <td>3066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600009 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID  ItemID\n",
       "0             0       8\n",
       "1             0      11\n",
       "2             0      12\n",
       "3             1      37\n",
       "4             2      22\n",
       "...         ...     ...\n",
       "1600004   69877    1646\n",
       "1600005   69877    1660\n",
       "1600006   69877    2001\n",
       "1600007   69877    2941\n",
       "1600008   69877    3066\n",
       "\n",
       "[1600009 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_validation_coo = sps.coo_matrix(URM_validation)\n",
    "\n",
    "correct_recommendations = pd.DataFrame({\"UserID\": URM_validation_coo.row,\n",
    "                                        \"ItemID\": URM_validation_coo.col})\n",
    "correct_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096335</th>\n",
       "      <td>69877</td>\n",
       "      <td>1283</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096336</th>\n",
       "      <td>69877</td>\n",
       "      <td>1429</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096337</th>\n",
       "      <td>69877</td>\n",
       "      <td>137</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096338</th>\n",
       "      <td>69877</td>\n",
       "      <td>1079</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096339</th>\n",
       "      <td>69877</td>\n",
       "      <td>411</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID      Exist\n",
       "0             0     94  left_only\n",
       "1             0     11       both\n",
       "2             0    175  left_only\n",
       "3             0     91  left_only\n",
       "4             0     85  left_only\n",
       "...         ...    ...        ...\n",
       "2096335   69877   1283  left_only\n",
       "2096336   69877   1429  left_only\n",
       "2096337   69877    137  left_only\n",
       "2096338   69877   1079  left_only\n",
       "2096339   69877    411  left_only\n",
       "\n",
       "[2096340 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = pd.merge(training_dataframe, correct_recommendations, on=['UserID','ItemID'], how='left', indicator='Exist')\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096335</th>\n",
       "      <td>69877</td>\n",
       "      <td>1283</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096336</th>\n",
       "      <td>69877</td>\n",
       "      <td>1429</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096337</th>\n",
       "      <td>69877</td>\n",
       "      <td>137</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096338</th>\n",
       "      <td>69877</td>\n",
       "      <td>1079</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096339</th>\n",
       "      <td>69877</td>\n",
       "      <td>411</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID  Label\n",
       "0             0     94  False\n",
       "1             0     11   True\n",
       "2             0    175  False\n",
       "3             0     91  False\n",
       "4             0     85  False\n",
       "...         ...    ...    ...\n",
       "2096335   69877   1283  False\n",
       "2096336   69877   1429  False\n",
       "2096337   69877    137  False\n",
       "2096338   69877   1079  False\n",
       "2096339   69877    411  False\n",
       "\n",
       "[2096340 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe[\"Label\"] = training_dataframe[\"Exist\"] == \"both\"\n",
    "training_dataframe.drop(columns = ['Exist'], inplace=True)\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's add some features. For example, the prediction of other algorithms\n",
    "\n",
    "This may take some time so it is a good idea to save this data and load it instead of calculating the scores every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopPopRecommender: URM Detected 71 ( 0.7%) items with no interactions.\n",
      "P3alphaRecommender: URM Detected 71 ( 0.7%) items with no interactions.\n",
      "P3alphaRecommender: Similarity column 10681 (100.0%), 1135.76 column/sec. Elapsed time 9.40 sec\n",
      "SLIM_BPR_Recommender: URM Detected 71 ( 0.7%) items with no interactions.\n",
      "Unable to read memory status: list index out of range\n",
      "SLIM_BPR_Recommender: Automatic selection of fastest train mode. Unable to get current RAM status, you may be using a non-Linux operating system. Using dense matrix.\n",
      "Processed 69878 (100.0%) in 1.50 sec. BPR loss is 3.64E-04. Sample per second: 46456\n",
      "SLIM_BPR_Recommender: Epoch 1 of 300. Elapsed time 0.59 sec\n",
      "Processed 69878 (100.0%) in 1.05 sec. BPR loss is 1.13E-03. Sample per second: 66575\n",
      "SLIM_BPR_Recommender: Epoch 2 of 300. Elapsed time 1.13 sec\n",
      "Processed 69878 (100.0%) in 0.56 sec. BPR loss is 1.94E-03. Sample per second: 124319\n",
      "SLIM_BPR_Recommender: Epoch 3 of 300. Elapsed time 1.64 sec\n",
      "Processed 69878 (100.0%) in 1.08 sec. BPR loss is 2.71E-03. Sample per second: 64908\n",
      "SLIM_BPR_Recommender: Epoch 4 of 300. Elapsed time 2.16 sec\n",
      "Processed 69878 (100.0%) in 0.68 sec. BPR loss is 3.51E-03. Sample per second: 103225\n",
      "SLIM_BPR_Recommender: Epoch 5 of 300. Elapsed time 2.76 sec\n",
      "Processed 69878 (100.0%) in 1.18 sec. BPR loss is 4.35E-03. Sample per second: 59296\n",
      "SLIM_BPR_Recommender: Epoch 6 of 300. Elapsed time 3.26 sec\n",
      "Processed 69878 (100.0%) in 0.83 sec. BPR loss is 5.17E-03. Sample per second: 84315\n",
      "SLIM_BPR_Recommender: Epoch 7 of 300. Elapsed time 3.91 sec\n",
      "Processed 69878 (100.0%) in 1.32 sec. BPR loss is 5.94E-03. Sample per second: 52967\n",
      "SLIM_BPR_Recommender: Epoch 8 of 300. Elapsed time 4.40 sec\n",
      "Processed 69878 (100.0%) in 0.90 sec. BPR loss is 6.73E-03. Sample per second: 77843\n",
      "SLIM_BPR_Recommender: Epoch 9 of 300. Elapsed time 4.98 sec\n",
      "Processed 69878 (100.0%) in 1.37 sec. BPR loss is 7.53E-03. Sample per second: 50998\n",
      "SLIM_BPR_Recommender: Epoch 10 of 300. Elapsed time 5.45 sec\n",
      "Processed 69878 (100.0%) in 0.99 sec. BPR loss is 8.49E-03. Sample per second: 70260\n",
      "SLIM_BPR_Recommender: Epoch 11 of 300. Elapsed time 6.08 sec\n",
      "Processed 69878 (100.0%) in 1.47 sec. BPR loss is 9.24E-03. Sample per second: 47566\n",
      "SLIM_BPR_Recommender: Epoch 12 of 300. Elapsed time 6.55 sec\n",
      "Processed 69878 (100.0%) in 0.99 sec. BPR loss is 9.87E-03. Sample per second: 70262\n",
      "SLIM_BPR_Recommender: Epoch 13 of 300. Elapsed time 7.08 sec\n",
      "Processed 69878 (100.0%) in 1.46 sec. BPR loss is 1.06E-02. Sample per second: 47761\n",
      "SLIM_BPR_Recommender: Epoch 14 of 300. Elapsed time 7.55 sec\n",
      "Processed 69878 (100.0%) in 0.92 sec. BPR loss is 1.13E-02. Sample per second: 76319\n",
      "SLIM_BPR_Recommender: Epoch 15 of 300. Elapsed time 8.00 sec\n",
      "Processed 69878 (100.0%) in 1.39 sec. BPR loss is 1.24E-02. Sample per second: 50340\n",
      "SLIM_BPR_Recommender: Epoch 16 of 300. Elapsed time 8.47 sec\n",
      "Processed 69878 (100.0%) in 0.89 sec. BPR loss is 1.29E-02. Sample per second: 78727\n",
      "SLIM_BPR_Recommender: Epoch 17 of 300. Elapsed time 8.97 sec\n",
      "Processed 69878 (100.0%) in 1.38 sec. BPR loss is 1.39E-02. Sample per second: 50596\n",
      "SLIM_BPR_Recommender: Epoch 18 of 300. Elapsed time 9.46 sec\n",
      "Processed 69878 (100.0%) in 0.87 sec. BPR loss is 1.47E-02. Sample per second: 80357\n",
      "SLIM_BPR_Recommender: Epoch 19 of 300. Elapsed time 9.95 sec\n",
      "Processed 69878 (100.0%) in 1.36 sec. BPR loss is 1.54E-02. Sample per second: 51377\n",
      "SLIM_BPR_Recommender: Epoch 20 of 300. Elapsed time 10.44 sec\n",
      "Processed 69878 (100.0%) in 0.82 sec. BPR loss is 1.61E-02. Sample per second: 84946\n",
      "SLIM_BPR_Recommender: Epoch 21 of 300. Elapsed time 10.90 sec\n",
      "Processed 69878 (100.0%) in 1.39 sec. BPR loss is 1.66E-02. Sample per second: 50198\n",
      "SLIM_BPR_Recommender: Epoch 22 of 300. Elapsed time 11.47 sec\n",
      "Processed 69878 (100.0%) in 0.89 sec. BPR loss is 1.80E-02. Sample per second: 78249\n",
      "SLIM_BPR_Recommender: Epoch 23 of 300. Elapsed time 11.98 sec\n",
      "Processed 69878 (100.0%) in 1.43 sec. BPR loss is 1.83E-02. Sample per second: 48986\n",
      "SLIM_BPR_Recommender: Epoch 24 of 300. Elapsed time 12.51 sec\n",
      "Processed 69878 (100.0%) in 1.11 sec. BPR loss is 1.93E-02. Sample per second: 63193\n",
      "SLIM_BPR_Recommender: Epoch 25 of 300. Elapsed time 13.19 sec\n",
      "Processed 69878 (100.0%) in 0.79 sec. BPR loss is 2.01E-02. Sample per second: 88220\n",
      "SLIM_BPR_Recommender: Epoch 26 of 300. Elapsed time 13.87 sec\n",
      "Processed 69878 (100.0%) in 1.42 sec. BPR loss is 2.08E-02. Sample per second: 49228\n",
      "SLIM_BPR_Recommender: Epoch 27 of 300. Elapsed time 14.50 sec\n",
      "Processed 69878 (100.0%) in 0.94 sec. BPR loss is 2.16E-02. Sample per second: 74500\n",
      "SLIM_BPR_Recommender: Epoch 28 of 300. Elapsed time 15.02 sec\n",
      "Processed 69878 (100.0%) in 1.50 sec. BPR loss is 2.21E-02. Sample per second: 46558\n",
      "SLIM_BPR_Recommender: Epoch 29 of 300. Elapsed time 15.58 sec\n",
      "Processed 69878 (100.0%) in 1.19 sec. BPR loss is 2.29E-02. Sample per second: 58709\n",
      "SLIM_BPR_Recommender: Epoch 30 of 300. Elapsed time 16.27 sec\n",
      "Processed 69878 (100.0%) in 0.88 sec. BPR loss is 2.37E-02. Sample per second: 79313\n",
      "SLIM_BPR_Recommender: Epoch 31 of 300. Elapsed time 16.96 sec\n",
      "Processed 69878 (100.0%) in 1.46 sec. BPR loss is 2.48E-02. Sample per second: 47994\n",
      "SLIM_BPR_Recommender: Epoch 32 of 300. Elapsed time 17.54 sec\n",
      "Processed 69878 (100.0%) in 1.07 sec. BPR loss is 2.50E-02. Sample per second: 65194\n",
      "SLIM_BPR_Recommender: Epoch 33 of 300. Elapsed time 18.15 sec\n",
      "Processed 69878 (100.0%) in 0.65 sec. BPR loss is 2.63E-02. Sample per second: 107292\n",
      "SLIM_BPR_Recommender: Epoch 34 of 300. Elapsed time 18.73 sec\n",
      "Processed 69878 (100.0%) in 1.14 sec. BPR loss is 2.67E-02. Sample per second: 61278\n",
      "SLIM_BPR_Recommender: Epoch 35 of 300. Elapsed time 19.22 sec\n",
      "Processed 69878 (100.0%) in 0.62 sec. BPR loss is 2.77E-02. Sample per second: 113081\n",
      "SLIM_BPR_Recommender: Epoch 36 of 300. Elapsed time 19.70 sec\n",
      "Processed 69878 (100.0%) in 1.25 sec. BPR loss is 2.89E-02. Sample per second: 55768\n",
      "SLIM_BPR_Recommender: Epoch 37 of 300. Elapsed time 20.34 sec\n",
      "Processed 69878 (100.0%) in 0.74 sec. BPR loss is 2.90E-02. Sample per second: 93791\n",
      "SLIM_BPR_Recommender: Epoch 38 of 300. Elapsed time 20.83 sec\n",
      "Processed 69878 (100.0%) in 1.24 sec. BPR loss is 3.02E-02. Sample per second: 56124\n",
      "SLIM_BPR_Recommender: Epoch 39 of 300. Elapsed time 21.33 sec\n",
      "Processed 69878 (100.0%) in 0.74 sec. BPR loss is 3.05E-02. Sample per second: 94932\n",
      "SLIM_BPR_Recommender: Epoch 40 of 300. Elapsed time 21.82 sec\n",
      "Processed 69878 (100.0%) in 1.25 sec. BPR loss is 3.15E-02. Sample per second: 55933\n",
      "SLIM_BPR_Recommender: Epoch 41 of 300. Elapsed time 22.33 sec\n",
      "Processed 69878 (100.0%) in 0.73 sec. BPR loss is 3.21E-02. Sample per second: 94949\n",
      "SLIM_BPR_Recommender: Epoch 42 of 300. Elapsed time 22.82 sec\n",
      "Processed 69878 (100.0%) in 1.21 sec. BPR loss is 3.26E-02. Sample per second: 57797\n",
      "SLIM_BPR_Recommender: Epoch 43 of 300. Elapsed time 23.29 sec\n",
      "Processed 69878 (100.0%) in 0.70 sec. BPR loss is 3.36E-02. Sample per second: 99244\n",
      "SLIM_BPR_Recommender: Epoch 44 of 300. Elapsed time 23.79 sec\n",
      "Processed 69878 (100.0%) in 1.19 sec. BPR loss is 3.41E-02. Sample per second: 58493\n",
      "SLIM_BPR_Recommender: Epoch 45 of 300. Elapsed time 24.28 sec\n",
      "Processed 69878 (100.0%) in 0.70 sec. BPR loss is 3.54E-02. Sample per second: 99951\n",
      "SLIM_BPR_Recommender: Epoch 46 of 300. Elapsed time 24.78 sec\n",
      "Processed 69878 (100.0%) in 1.18 sec. BPR loss is 3.62E-02. Sample per second: 59388\n",
      "SLIM_BPR_Recommender: Epoch 47 of 300. Elapsed time 25.26 sec\n",
      "Processed 69878 (100.0%) in 0.64 sec. BPR loss is 3.73E-02. Sample per second: 108988\n",
      "SLIM_BPR_Recommender: Epoch 48 of 300. Elapsed time 25.72 sec\n",
      "Processed 69878 (100.0%) in 1.14 sec. BPR loss is 3.79E-02. Sample per second: 61369\n",
      "SLIM_BPR_Recommender: Epoch 49 of 300. Elapsed time 26.22 sec\n",
      "Processed 69878 (100.0%) in 0.60 sec. BPR loss is 3.84E-02. Sample per second: 116430\n",
      "SLIM_BPR_Recommender: Epoch 50 of 300. Elapsed time 26.68 sec\n",
      "Processed 69878 (100.0%) in 1.06 sec. BPR loss is 3.89E-02. Sample per second: 65815\n",
      "SLIM_BPR_Recommender: Epoch 51 of 300. Elapsed time 27.14 sec\n",
      "Processed 69878 (100.0%) in 0.55 sec. BPR loss is 3.96E-02. Sample per second: 126316\n",
      "SLIM_BPR_Recommender: Epoch 52 of 300. Elapsed time 27.64 sec\n",
      "Processed 69878 (100.0%) in 1.02 sec. BPR loss is 4.09E-02. Sample per second: 68327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: Epoch 53 of 300. Elapsed time 28.10 sec\n",
      "Processed 69878 (100.0%) in 0.51 sec. BPR loss is 4.11E-02. Sample per second: 138037\n",
      "SLIM_BPR_Recommender: Epoch 54 of 300. Elapsed time 28.59 sec\n",
      "Processed 69878 (100.0%) in 0.97 sec. BPR loss is 4.15E-02. Sample per second: 71836\n",
      "SLIM_BPR_Recommender: Epoch 55 of 300. Elapsed time 29.05 sec\n",
      "Processed 69878 (100.0%) in 1.43 sec. BPR loss is 4.29E-02. Sample per second: 48754\n",
      "SLIM_BPR_Recommender: Epoch 56 of 300. Elapsed time 29.52 sec\n",
      "Processed 69878 (100.0%) in 0.92 sec. BPR loss is 4.34E-02. Sample per second: 76137\n",
      "SLIM_BPR_Recommender: Epoch 57 of 300. Elapsed time 30.00 sec\n",
      "Processed 69878 (100.0%) in 1.41 sec. BPR loss is 4.47E-02. Sample per second: 49478\n",
      "SLIM_BPR_Recommender: Epoch 58 of 300. Elapsed time 30.49 sec\n",
      "Processed 69878 (100.0%) in 0.89 sec. BPR loss is 4.50E-02. Sample per second: 78269\n",
      "SLIM_BPR_Recommender: Epoch 59 of 300. Elapsed time 30.98 sec\n",
      "Processed 69878 (100.0%) in 1.39 sec. BPR loss is 4.56E-02. Sample per second: 50226\n",
      "SLIM_BPR_Recommender: Epoch 60 of 300. Elapsed time 31.47 sec\n",
      "Processed 69878 (100.0%) in 0.85 sec. BPR loss is 4.69E-02. Sample per second: 81822\n",
      "SLIM_BPR_Recommender: Epoch 61 of 300. Elapsed time 31.94 sec\n",
      "Processed 69878 (100.0%) in 1.32 sec. BPR loss is 4.72E-02. Sample per second: 53036\n",
      "SLIM_BPR_Recommender: Epoch 62 of 300. Elapsed time 32.40 sec\n",
      "Processed 69878 (100.0%) in 0.78 sec. BPR loss is 4.72E-02. Sample per second: 89347\n",
      "SLIM_BPR_Recommender: Epoch 63 of 300. Elapsed time 32.86 sec\n",
      "Processed 69878 (100.0%) in 1.26 sec. BPR loss is 4.82E-02. Sample per second: 55316\n",
      "SLIM_BPR_Recommender: Epoch 64 of 300. Elapsed time 33.35 sec\n",
      "Processed 69878 (100.0%) in 0.72 sec. BPR loss is 4.87E-02. Sample per second: 96676\n",
      "SLIM_BPR_Recommender: Epoch 65 of 300. Elapsed time 33.81 sec\n",
      "Processed 69878 (100.0%) in 1.19 sec. BPR loss is 4.97E-02. Sample per second: 58655\n",
      "SLIM_BPR_Recommender: Epoch 66 of 300. Elapsed time 34.27 sec\n",
      "Processed 69878 (100.0%) in 0.65 sec. BPR loss is 5.08E-02. Sample per second: 107037\n",
      "SLIM_BPR_Recommender: Epoch 67 of 300. Elapsed time 34.74 sec\n",
      "Processed 69878 (100.0%) in 1.11 sec. BPR loss is 5.15E-02. Sample per second: 62817\n",
      "SLIM_BPR_Recommender: Epoch 68 of 300. Elapsed time 35.19 sec\n",
      "Processed 69878 (100.0%) in 0.57 sec. BPR loss is 5.21E-02. Sample per second: 123262\n",
      "SLIM_BPR_Recommender: Epoch 69 of 300. Elapsed time 35.65 sec\n",
      "Processed 69878 (100.0%) in 1.02 sec. BPR loss is 5.25E-02. Sample per second: 68411\n",
      "SLIM_BPR_Recommender: Epoch 70 of 300. Elapsed time 36.10 sec\n",
      "Processed 69878 (100.0%) in 0.48 sec. BPR loss is 5.40E-02. Sample per second: 145044\n",
      "SLIM_BPR_Recommender: Epoch 71 of 300. Elapsed time 36.56 sec\n",
      "Processed 69878 (100.0%) in 0.93 sec. BPR loss is 5.41E-02. Sample per second: 74748\n",
      "SLIM_BPR_Recommender: Epoch 72 of 300. Elapsed time 37.02 sec\n",
      "Processed 69878 (100.0%) in 1.40 sec. BPR loss is 5.52E-02. Sample per second: 49899\n",
      "SLIM_BPR_Recommender: Epoch 73 of 300. Elapsed time 37.48 sec\n",
      "Processed 69878 (100.0%) in 0.85 sec. BPR loss is 5.57E-02. Sample per second: 81642\n",
      "SLIM_BPR_Recommender: Epoch 74 of 300. Elapsed time 37.94 sec\n",
      "Processed 69878 (100.0%) in 1.32 sec. BPR loss is 5.68E-02. Sample per second: 53081\n",
      "SLIM_BPR_Recommender: Epoch 75 of 300. Elapsed time 38.40 sec\n",
      "Processed 69878 (100.0%) in 0.80 sec. BPR loss is 5.72E-02. Sample per second: 87572\n",
      "SLIM_BPR_Recommender: Epoch 76 of 300. Elapsed time 38.88 sec\n",
      "Processed 69878 (100.0%) in 1.26 sec. BPR loss is 5.72E-02. Sample per second: 55481\n",
      "SLIM_BPR_Recommender: Epoch 77 of 300. Elapsed time 39.34 sec\n",
      "Processed 69878 (100.0%) in 0.72 sec. BPR loss is 5.90E-02. Sample per second: 97730\n",
      "SLIM_BPR_Recommender: Epoch 78 of 300. Elapsed time 39.80 sec\n",
      "Processed 69878 (100.0%) in 1.18 sec. BPR loss is 5.88E-02. Sample per second: 59443\n",
      "SLIM_BPR_Recommender: Epoch 79 of 300. Elapsed time 40.26 sec\n",
      "Processed 69878 (100.0%) in 0.64 sec. BPR loss is 6.00E-02. Sample per second: 109684\n",
      "SLIM_BPR_Recommender: Epoch 80 of 300. Elapsed time 40.72 sec\n",
      "Processed 69878 (100.0%) in 1.15 sec. BPR loss is 6.05E-02. Sample per second: 60734\n",
      "SLIM_BPR_Recommender: Epoch 81 of 300. Elapsed time 41.23 sec\n",
      "Processed 69878 (100.0%) in 0.61 sec. BPR loss is 6.15E-02. Sample per second: 114353\n",
      "SLIM_BPR_Recommender: Epoch 82 of 300. Elapsed time 41.69 sec\n",
      "Processed 69878 (100.0%) in 1.07 sec. BPR loss is 6.32E-02. Sample per second: 65453\n",
      "SLIM_BPR_Recommender: Epoch 83 of 300. Elapsed time 42.15 sec\n",
      "Processed 69878 (100.0%) in 0.61 sec. BPR loss is 6.27E-02. Sample per second: 114357\n",
      "SLIM_BPR_Recommender: Epoch 84 of 300. Elapsed time 42.69 sec\n",
      "Processed 69878 (100.0%) in 1.07 sec. BPR loss is 6.31E-02. Sample per second: 65515\n",
      "SLIM_BPR_Recommender: Epoch 85 of 300. Elapsed time 43.15 sec\n",
      "Processed 69878 (100.0%) in 0.55 sec. BPR loss is 6.46E-02. Sample per second: 126338\n",
      "SLIM_BPR_Recommender: Epoch 86 of 300. Elapsed time 43.64 sec\n",
      "Processed 69878 (100.0%) in 1.01 sec. BPR loss is 6.55E-02. Sample per second: 68870\n",
      "SLIM_BPR_Recommender: Epoch 87 of 300. Elapsed time 44.10 sec\n",
      "Processed 69878 (100.0%) in 0.48 sec. BPR loss is 6.54E-02. Sample per second: 144335\n",
      "SLIM_BPR_Recommender: Epoch 88 of 300. Elapsed time 44.57 sec\n",
      "Processed 69878 (100.0%) in 0.94 sec. BPR loss is 6.66E-02. Sample per second: 74048\n",
      "SLIM_BPR_Recommender: Epoch 89 of 300. Elapsed time 45.03 sec\n",
      "Processed 69878 (100.0%) in 1.41 sec. BPR loss is 6.67E-02. Sample per second: 49517\n",
      "SLIM_BPR_Recommender: Epoch 90 of 300. Elapsed time 45.49 sec\n",
      "Processed 69878 (100.0%) in 0.87 sec. BPR loss is 6.85E-02. Sample per second: 80068\n",
      "SLIM_BPR_Recommender: Epoch 91 of 300. Elapsed time 45.95 sec\n",
      "Processed 69878 (100.0%) in 1.33 sec. BPR loss is 6.92E-02. Sample per second: 52561\n",
      "SLIM_BPR_Recommender: Epoch 92 of 300. Elapsed time 46.41 sec\n",
      "Processed 69878 (100.0%) in 0.78 sec. BPR loss is 6.93E-02. Sample per second: 89243\n",
      "SLIM_BPR_Recommender: Epoch 93 of 300. Elapsed time 46.87 sec\n",
      "Processed 69878 (100.0%) in 1.24 sec. BPR loss is 6.95E-02. Sample per second: 56148\n",
      "SLIM_BPR_Recommender: Epoch 94 of 300. Elapsed time 47.33 sec\n",
      "Processed 69878 (100.0%) in 0.73 sec. BPR loss is 7.06E-02. Sample per second: 95980\n",
      "SLIM_BPR_Recommender: Epoch 95 of 300. Elapsed time 47.81 sec\n",
      "Processed 69878 (100.0%) in 1.19 sec. BPR loss is 7.19E-02. Sample per second: 58791\n",
      "SLIM_BPR_Recommender: Epoch 96 of 300. Elapsed time 48.27 sec\n",
      "Processed 69878 (100.0%) in 0.66 sec. BPR loss is 7.21E-02. Sample per second: 106669\n",
      "SLIM_BPR_Recommender: Epoch 97 of 300. Elapsed time 48.74 sec\n",
      "Processed 69878 (100.0%) in 1.12 sec. BPR loss is 7.31E-02. Sample per second: 62135\n",
      "SLIM_BPR_Recommender: Epoch 98 of 300. Elapsed time 49.21 sec\n",
      "Processed 69878 (100.0%) in 0.59 sec. BPR loss is 7.33E-02. Sample per second: 119213\n",
      "SLIM_BPR_Recommender: Epoch 99 of 300. Elapsed time 49.67 sec\n",
      "Processed 69878 (100.0%) in 1.05 sec. BPR loss is 7.48E-02. Sample per second: 66698\n",
      "SLIM_BPR_Recommender: Epoch 100 of 300. Elapsed time 50.13 sec\n",
      "Processed 69878 (100.0%) in 0.51 sec. BPR loss is 7.46E-02. Sample per second: 137771\n",
      "SLIM_BPR_Recommender: Epoch 101 of 300. Elapsed time 50.59 sec\n",
      "Processed 69878 (100.0%) in 0.96 sec. BPR loss is 7.58E-02. Sample per second: 72433\n",
      "SLIM_BPR_Recommender: Epoch 102 of 300. Elapsed time 51.05 sec\n",
      "Processed 69878 (100.0%) in 1.44 sec. BPR loss is 7.66E-02. Sample per second: 48451\n",
      "SLIM_BPR_Recommender: Epoch 103 of 300. Elapsed time 51.52 sec\n",
      "Processed 69878 (100.0%) in 0.92 sec. BPR loss is 7.77E-02. Sample per second: 75564\n",
      "SLIM_BPR_Recommender: Epoch 104 of 300. Elapsed time 52.01 sec\n",
      "Processed 69878 (100.0%) in 1.38 sec. BPR loss is 7.75E-02. Sample per second: 50480\n",
      "SLIM_BPR_Recommender: Epoch 105 of 300. Elapsed time 52.47 sec\n",
      "Processed 69878 (100.0%) in 0.85 sec. BPR loss is 7.88E-02. Sample per second: 81748\n",
      "SLIM_BPR_Recommender: Epoch 106 of 300. Elapsed time 52.94 sec\n",
      "Processed 69878 (100.0%) in 1.36 sec. BPR loss is 7.93E-02. Sample per second: 51332\n",
      "SLIM_BPR_Recommender: Epoch 107 of 300. Elapsed time 53.44 sec\n",
      "Processed 69878 (100.0%) in 0.88 sec. BPR loss is 7.89E-02. Sample per second: 79429\n",
      "SLIM_BPR_Recommender: Epoch 108 of 300. Elapsed time 53.96 sec\n",
      "Processed 69878 (100.0%) in 1.35 sec. BPR loss is 8.13E-02. Sample per second: 51905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: Epoch 109 of 300. Elapsed time 54.43 sec\n",
      "Processed 69878 (100.0%) in 0.80 sec. BPR loss is 8.06E-02. Sample per second: 86824\n",
      "SLIM_BPR_Recommender: Epoch 110 of 300. Elapsed time 54.89 sec\n",
      "Processed 69878 (100.0%) in 1.29 sec. BPR loss is 8.26E-02. Sample per second: 54114\n",
      "SLIM_BPR_Recommender: Epoch 111 of 300. Elapsed time 55.37 sec\n",
      "Processed 69878 (100.0%) in 0.76 sec. BPR loss is 8.36E-02. Sample per second: 91602\n",
      "SLIM_BPR_Recommender: Epoch 112 of 300. Elapsed time 55.84 sec\n",
      "Processed 69878 (100.0%) in 1.24 sec. BPR loss is 8.26E-02. Sample per second: 56292\n",
      "SLIM_BPR_Recommender: Epoch 113 of 300. Elapsed time 56.32 sec\n",
      "Processed 69878 (100.0%) in 0.71 sec. BPR loss is 8.51E-02. Sample per second: 97889\n",
      "SLIM_BPR_Recommender: Epoch 114 of 300. Elapsed time 56.80 sec\n",
      "Processed 69878 (100.0%) in 1.20 sec. BPR loss is 8.42E-02. Sample per second: 58360\n",
      "SLIM_BPR_Recommender: Epoch 115 of 300. Elapsed time 57.28 sec\n",
      "Processed 69878 (100.0%) in 0.71 sec. BPR loss is 8.49E-02. Sample per second: 98442\n",
      "SLIM_BPR_Recommender: Epoch 116 of 300. Elapsed time 57.79 sec\n",
      "Processed 69878 (100.0%) in 1.17 sec. BPR loss is 8.60E-02. Sample per second: 59604\n",
      "SLIM_BPR_Recommender: Epoch 117 of 300. Elapsed time 58.25 sec\n",
      "Processed 69878 (100.0%) in 0.64 sec. BPR loss is 8.57E-02. Sample per second: 108528\n",
      "SLIM_BPR_Recommender: Epoch 118 of 300. Elapsed time 58.73 sec\n",
      "Processed 69878 (100.0%) in 1.12 sec. BPR loss is 8.78E-02. Sample per second: 62147\n",
      "SLIM_BPR_Recommender: Epoch 119 of 300. Elapsed time 59.21 sec\n",
      "Processed 69878 (100.0%) in 0.59 sec. BPR loss is 8.76E-02. Sample per second: 118656\n",
      "SLIM_BPR_Recommender: Epoch 120 of 300. Elapsed time 59.67 sec\n",
      "Processed 69878 (100.0%) in 1.05 sec. BPR loss is 8.99E-02. Sample per second: 66776\n",
      "SLIM_BPR_Recommender: Epoch 121 of 300. Elapsed time 1.00 min\n",
      "Processed 69878 (100.0%) in 0.53 sec. BPR loss is 9.01E-02. Sample per second: 130623\n",
      "SLIM_BPR_Recommender: Epoch 122 of 300. Elapsed time 1.01 min\n",
      "Processed 69878 (100.0%) in 1.00 sec. BPR loss is 9.18E-02. Sample per second: 69775\n",
      "SLIM_BPR_Recommender: Epoch 123 of 300. Elapsed time 1.02 min\n",
      "Processed 69878 (100.0%) in 0.46 sec. BPR loss is 9.16E-02. Sample per second: 151577\n",
      "SLIM_BPR_Recommender: Epoch 124 of 300. Elapsed time 1.03 min\n",
      "Processed 69878 (100.0%) in 0.92 sec. BPR loss is 9.25E-02. Sample per second: 75582\n",
      "SLIM_BPR_Recommender: Epoch 125 of 300. Elapsed time 1.03 min\n",
      "Processed 69878 (100.0%) in 1.38 sec. BPR loss is 9.32E-02. Sample per second: 50523\n",
      "SLIM_BPR_Recommender: Epoch 126 of 300. Elapsed time 1.04 min\n",
      "Processed 69878 (100.0%) in 0.84 sec. BPR loss is 9.34E-02. Sample per second: 82931\n",
      "SLIM_BPR_Recommender: Epoch 127 of 300. Elapsed time 1.05 min\n",
      "Processed 69878 (100.0%) in 1.30 sec. BPR loss is 9.34E-02. Sample per second: 53871\n",
      "SLIM_BPR_Recommender: Epoch 128 of 300. Elapsed time 1.06 min\n",
      "Processed 69878 (100.0%) in 0.76 sec. BPR loss is 9.38E-02. Sample per second: 92107\n",
      "SLIM_BPR_Recommender: Epoch 129 of 300. Elapsed time 1.06 min\n",
      "Processed 69878 (100.0%) in 1.22 sec. BPR loss is 9.42E-02. Sample per second: 57315\n",
      "SLIM_BPR_Recommender: Epoch 130 of 300. Elapsed time 1.07 min\n",
      "Processed 69878 (100.0%) in 0.68 sec. BPR loss is 9.57E-02. Sample per second: 103413\n",
      "SLIM_BPR_Recommender: Epoch 131 of 300. Elapsed time 1.08 min\n",
      "Processed 69878 (100.0%) in 1.17 sec. BPR loss is 9.70E-02. Sample per second: 59461\n",
      "SLIM_BPR_Recommender: Epoch 132 of 300. Elapsed time 1.09 min\n",
      "Processed 69878 (100.0%) in 0.64 sec. BPR loss is 9.75E-02. Sample per second: 109401\n",
      "SLIM_BPR_Recommender: Epoch 133 of 300. Elapsed time 1.10 min\n",
      "Processed 69878 (100.0%) in 1.12 sec. BPR loss is 9.76E-02. Sample per second: 62545\n",
      "SLIM_BPR_Recommender: Epoch 134 of 300. Elapsed time 1.10 min\n",
      "Processed 69878 (100.0%) in 0.59 sec. BPR loss is 9.82E-02. Sample per second: 118686\n",
      "SLIM_BPR_Recommender: Epoch 135 of 300. Elapsed time 1.11 min\n",
      "Processed 69878 (100.0%) in 1.08 sec. BPR loss is 9.79E-02. Sample per second: 64805\n",
      "SLIM_BPR_Recommender: Epoch 136 of 300. Elapsed time 1.12 min\n",
      "Processed 69878 (100.0%) in 0.54 sec. BPR loss is 9.98E-02. Sample per second: 129938\n",
      "SLIM_BPR_Recommender: Epoch 137 of 300. Elapsed time 1.13 min\n",
      "Processed 69878 (100.0%) in 1.00 sec. BPR loss is 1.01E-01. Sample per second: 70065\n",
      "SLIM_BPR_Recommender: Epoch 138 of 300. Elapsed time 1.13 min\n",
      "Processed 69878 (100.0%) in 1.46 sec. BPR loss is 9.97E-02. Sample per second: 47703\n",
      "SLIM_BPR_Recommender: Epoch 139 of 300. Elapsed time 1.14 min\n",
      "Processed 69878 (100.0%) in 0.96 sec. BPR loss is 1.03E-01. Sample per second: 72614\n",
      "SLIM_BPR_Recommender: Epoch 140 of 300. Elapsed time 1.15 min\n",
      "Processed 69878 (100.0%) in 1.43 sec. BPR loss is 1.02E-01. Sample per second: 48973\n",
      "SLIM_BPR_Recommender: Epoch 141 of 300. Elapsed time 1.16 min\n",
      "Processed 69878 (100.0%) in 0.89 sec. BPR loss is 1.02E-01. Sample per second: 78923\n",
      "SLIM_BPR_Recommender: Epoch 142 of 300. Elapsed time 1.17 min\n",
      "Processed 69878 (100.0%) in 1.36 sec. BPR loss is 1.04E-01. Sample per second: 51497\n",
      "SLIM_BPR_Recommender: Epoch 143 of 300. Elapsed time 1.17 min\n",
      "Processed 69878 (100.0%) in 0.82 sec. BPR loss is 1.06E-01. Sample per second: 85067\n",
      "SLIM_BPR_Recommender: Epoch 144 of 300. Elapsed time 1.18 min\n",
      "Processed 69878 (100.0%) in 1.28 sec. BPR loss is 1.05E-01. Sample per second: 54466\n",
      "SLIM_BPR_Recommender: Epoch 145 of 300. Elapsed time 1.19 min\n",
      "Processed 69878 (100.0%) in 0.77 sec. BPR loss is 1.08E-01. Sample per second: 90463\n",
      "SLIM_BPR_Recommender: Epoch 146 of 300. Elapsed time 1.20 min\n",
      "Processed 69878 (100.0%) in 1.25 sec. BPR loss is 1.05E-01. Sample per second: 55993\n",
      "SLIM_BPR_Recommender: Epoch 147 of 300. Elapsed time 1.21 min\n",
      "Processed 69878 (100.0%) in 0.71 sec. BPR loss is 1.06E-01. Sample per second: 98486\n",
      "SLIM_BPR_Recommender: Epoch 148 of 300. Elapsed time 1.21 min\n",
      "Processed 69878 (100.0%) in 1.17 sec. BPR loss is 1.07E-01. Sample per second: 59812\n",
      "SLIM_BPR_Recommender: Epoch 149 of 300. Elapsed time 1.22 min\n",
      "Processed 69878 (100.0%) in 0.63 sec. BPR loss is 1.08E-01. Sample per second: 110770\n",
      "SLIM_BPR_Recommender: Epoch 150 of 300. Elapsed time 1.23 min\n",
      "Processed 69878 (100.0%) in 1.08 sec. BPR loss is 1.09E-01. Sample per second: 64441\n",
      "SLIM_BPR_Recommender: Epoch 151 of 300. Elapsed time 1.24 min\n",
      "Processed 69878 (100.0%) in 0.55 sec. BPR loss is 1.11E-01. Sample per second: 125931\n",
      "SLIM_BPR_Recommender: Epoch 152 of 300. Elapsed time 1.24 min\n",
      "Processed 69878 (100.0%) in 1.01 sec. BPR loss is 1.10E-01. Sample per second: 69158\n",
      "SLIM_BPR_Recommender: Epoch 153 of 300. Elapsed time 1.25 min\n",
      "Processed 69878 (100.0%) in 0.47 sec. BPR loss is 1.11E-01. Sample per second: 149649\n",
      "SLIM_BPR_Recommender: Epoch 154 of 300. Elapsed time 1.26 min\n",
      "Processed 69878 (100.0%) in 0.92 sec. BPR loss is 1.12E-01. Sample per second: 75947\n",
      "SLIM_BPR_Recommender: Epoch 155 of 300. Elapsed time 1.27 min\n",
      "Processed 69878 (100.0%) in 1.37 sec. BPR loss is 1.12E-01. Sample per second: 50945\n",
      "SLIM_BPR_Recommender: Epoch 156 of 300. Elapsed time 1.27 min\n",
      "Processed 69878 (100.0%) in 0.84 sec. BPR loss is 1.13E-01. Sample per second: 82976\n",
      "SLIM_BPR_Recommender: Epoch 157 of 300. Elapsed time 1.28 min\n",
      "Processed 69878 (100.0%) in 1.30 sec. BPR loss is 1.13E-01. Sample per second: 53601\n",
      "SLIM_BPR_Recommender: Epoch 158 of 300. Elapsed time 1.29 min\n",
      "Processed 69878 (100.0%) in 0.77 sec. BPR loss is 1.15E-01. Sample per second: 90376\n",
      "SLIM_BPR_Recommender: Epoch 159 of 300. Elapsed time 1.30 min\n",
      "Processed 69878 (100.0%) in 1.23 sec. BPR loss is 1.15E-01. Sample per second: 56639\n",
      "SLIM_BPR_Recommender: Epoch 160 of 300. Elapsed time 1.31 min\n",
      "Processed 69878 (100.0%) in 0.69 sec. BPR loss is 1.16E-01. Sample per second: 100943\n",
      "SLIM_BPR_Recommender: Epoch 161 of 300. Elapsed time 1.31 min\n",
      "Processed 69878 (100.0%) in 1.19 sec. BPR loss is 1.16E-01. Sample per second: 58635\n",
      "SLIM_BPR_Recommender: Epoch 162 of 300. Elapsed time 1.32 min\n",
      "Processed 69878 (100.0%) in 0.65 sec. BPR loss is 1.18E-01. Sample per second: 106964\n",
      "SLIM_BPR_Recommender: Epoch 163 of 300. Elapsed time 1.33 min\n",
      "Processed 69878 (100.0%) in 1.11 sec. BPR loss is 1.19E-01. Sample per second: 62850\n",
      "SLIM_BPR_Recommender: Epoch 164 of 300. Elapsed time 1.34 min\n",
      "Processed 69878 (100.0%) in 0.65 sec. BPR loss is 1.19E-01. Sample per second: 108292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: Epoch 165 of 300. Elapsed time 1.35 min\n",
      "Processed 69878 (100.0%) in 1.16 sec. BPR loss is 1.18E-01. Sample per second: 60254\n",
      "SLIM_BPR_Recommender: Epoch 166 of 300. Elapsed time 1.35 min\n",
      "Processed 69878 (100.0%) in 0.70 sec. BPR loss is 1.20E-01. Sample per second: 100370\n",
      "SLIM_BPR_Recommender: Epoch 167 of 300. Elapsed time 1.36 min\n",
      "Processed 69878 (100.0%) in 1.21 sec. BPR loss is 1.22E-01. Sample per second: 57766\n",
      "SLIM_BPR_Recommender: Epoch 168 of 300. Elapsed time 1.37 min\n",
      "Processed 69878 (100.0%) in 0.68 sec. BPR loss is 1.21E-01. Sample per second: 102733\n",
      "SLIM_BPR_Recommender: Epoch 169 of 300. Elapsed time 1.38 min\n",
      "Processed 69878 (100.0%) in 1.19 sec. BPR loss is 1.22E-01. Sample per second: 58737\n",
      "SLIM_BPR_Recommender: Epoch 170 of 300. Elapsed time 1.39 min\n",
      "Processed 69878 (100.0%) in 0.68 sec. BPR loss is 1.23E-01. Sample per second: 101989\n",
      "SLIM_BPR_Recommender: Epoch 171 of 300. Elapsed time 1.40 min\n",
      "Processed 69878 (100.0%) in 1.14 sec. BPR loss is 1.23E-01. Sample per second: 61099\n",
      "SLIM_BPR_Recommender: Epoch 172 of 300. Elapsed time 1.40 min\n",
      "Processed 69878 (100.0%) in 0.61 sec. BPR loss is 1.24E-01. Sample per second: 115267\n",
      "SLIM_BPR_Recommender: Epoch 173 of 300. Elapsed time 1.41 min\n",
      "Processed 69878 (100.0%) in 1.08 sec. BPR loss is 1.24E-01. Sample per second: 64480\n",
      "SLIM_BPR_Recommender: Epoch 174 of 300. Elapsed time 1.42 min\n",
      "Processed 69878 (100.0%) in 0.55 sec. BPR loss is 1.26E-01. Sample per second: 127920\n",
      "SLIM_BPR_Recommender: Epoch 175 of 300. Elapsed time 1.43 min\n",
      "Processed 69878 (100.0%) in 1.01 sec. BPR loss is 1.25E-01. Sample per second: 69269\n",
      "SLIM_BPR_Recommender: Epoch 176 of 300. Elapsed time 1.43 min\n",
      "Processed 69878 (100.0%) in 0.47 sec. BPR loss is 1.26E-01. Sample per second: 148580\n",
      "SLIM_BPR_Recommender: Epoch 177 of 300. Elapsed time 1.44 min\n",
      "Processed 69878 (100.0%) in 0.97 sec. BPR loss is 1.28E-01. Sample per second: 71831\n",
      "SLIM_BPR_Recommender: Epoch 178 of 300. Elapsed time 1.45 min\n",
      "Processed 69878 (100.0%) in 1.43 sec. BPR loss is 1.28E-01. Sample per second: 48685\n",
      "SLIM_BPR_Recommender: Epoch 179 of 300. Elapsed time 1.46 min\n",
      "Processed 69878 (100.0%) in 0.89 sec. BPR loss is 1.29E-01. Sample per second: 78351\n",
      "SLIM_BPR_Recommender: Epoch 180 of 300. Elapsed time 1.47 min\n",
      "Processed 69878 (100.0%) in 1.35 sec. BPR loss is 1.30E-01. Sample per second: 51746\n",
      "SLIM_BPR_Recommender: Epoch 181 of 300. Elapsed time 1.47 min\n",
      "Processed 69878 (100.0%) in 0.81 sec. BPR loss is 1.29E-01. Sample per second: 85926\n",
      "SLIM_BPR_Recommender: Epoch 182 of 300. Elapsed time 1.48 min\n",
      "Processed 69878 (100.0%) in 1.29 sec. BPR loss is 1.30E-01. Sample per second: 54264\n",
      "SLIM_BPR_Recommender: Epoch 183 of 300. Elapsed time 1.49 min\n",
      "Processed 69878 (100.0%) in 0.75 sec. BPR loss is 1.32E-01. Sample per second: 93261\n",
      "SLIM_BPR_Recommender: Epoch 184 of 300. Elapsed time 1.50 min\n",
      "Processed 69878 (100.0%) in 1.22 sec. BPR loss is 1.32E-01. Sample per second: 57147\n",
      "SLIM_BPR_Recommender: Epoch 185 of 300. Elapsed time 1.51 min\n",
      "Processed 69878 (100.0%) in 0.68 sec. BPR loss is 1.31E-01. Sample per second: 102565\n",
      "SLIM_BPR_Recommender: Epoch 186 of 300. Elapsed time 1.51 min\n",
      "Processed 69878 (100.0%) in 1.15 sec. BPR loss is 1.33E-01. Sample per second: 60720\n",
      "SLIM_BPR_Recommender: Epoch 187 of 300. Elapsed time 1.52 min\n",
      "Processed 69878 (100.0%) in 0.61 sec. BPR loss is 1.35E-01. Sample per second: 114675\n",
      "SLIM_BPR_Recommender: Epoch 188 of 300. Elapsed time 1.53 min\n",
      "Processed 69878 (100.0%) in 1.07 sec. BPR loss is 1.35E-01. Sample per second: 65070\n",
      "SLIM_BPR_Recommender: Epoch 189 of 300. Elapsed time 1.54 min\n",
      "Processed 69878 (100.0%) in 0.54 sec. BPR loss is 1.34E-01. Sample per second: 130270\n",
      "SLIM_BPR_Recommender: Epoch 190 of 300. Elapsed time 1.54 min\n",
      "Processed 69878 (100.0%) in 1.05 sec. BPR loss is 1.37E-01. Sample per second: 66557\n",
      "SLIM_BPR_Recommender: Epoch 191 of 300. Elapsed time 1.55 min\n",
      "Processed 69878 (100.0%) in 0.51 sec. BPR loss is 1.37E-01. Sample per second: 136633\n",
      "SLIM_BPR_Recommender: Epoch 192 of 300. Elapsed time 1.56 min\n",
      "Processed 69878 (100.0%) in 0.98 sec. BPR loss is 1.39E-01. Sample per second: 71164\n",
      "SLIM_BPR_Recommender: Epoch 193 of 300. Elapsed time 1.57 min\n",
      "Processed 69878 (100.0%) in 1.44 sec. BPR loss is 1.41E-01. Sample per second: 48612\n",
      "SLIM_BPR_Recommender: Epoch 194 of 300. Elapsed time 1.58 min\n",
      "Processed 69878 (100.0%) in 0.90 sec. BPR loss is 1.40E-01. Sample per second: 77902\n",
      "SLIM_BPR_Recommender: Epoch 195 of 300. Elapsed time 1.58 min\n",
      "Processed 69878 (100.0%) in 1.35 sec. BPR loss is 1.38E-01. Sample per second: 51817\n",
      "SLIM_BPR_Recommender: Epoch 196 of 300. Elapsed time 1.59 min\n",
      "Processed 69878 (100.0%) in 0.81 sec. BPR loss is 1.41E-01. Sample per second: 86798\n",
      "SLIM_BPR_Recommender: Epoch 197 of 300. Elapsed time 1.60 min\n",
      "Processed 69878 (100.0%) in 1.28 sec. BPR loss is 1.41E-01. Sample per second: 54524\n",
      "SLIM_BPR_Recommender: Epoch 198 of 300. Elapsed time 1.61 min\n",
      "Processed 69878 (100.0%) in 0.73 sec. BPR loss is 1.40E-01. Sample per second: 95186\n",
      "SLIM_BPR_Recommender: Epoch 199 of 300. Elapsed time 1.61 min\n",
      "Processed 69878 (100.0%) in 1.20 sec. BPR loss is 1.42E-01. Sample per second: 58395\n",
      "SLIM_BPR_Recommender: Epoch 200 of 300. Elapsed time 1.62 min\n",
      "Processed 69878 (100.0%) in 0.65 sec. BPR loss is 1.39E-01. Sample per second: 107145\n",
      "SLIM_BPR_Recommender: Epoch 201 of 300. Elapsed time 1.63 min\n",
      "Processed 69878 (100.0%) in 1.15 sec. BPR loss is 1.44E-01. Sample per second: 60834\n",
      "SLIM_BPR_Recommender: Epoch 202 of 300. Elapsed time 1.64 min\n",
      "Processed 69878 (100.0%) in 0.63 sec. BPR loss is 1.45E-01. Sample per second: 111236\n",
      "SLIM_BPR_Recommender: Epoch 203 of 300. Elapsed time 1.65 min\n",
      "Processed 69878 (100.0%) in 1.10 sec. BPR loss is 1.46E-01. Sample per second: 63658\n",
      "SLIM_BPR_Recommender: Epoch 204 of 300. Elapsed time 1.65 min\n",
      "Processed 69878 (100.0%) in 0.56 sec. BPR loss is 1.44E-01. Sample per second: 125398\n",
      "SLIM_BPR_Recommender: Epoch 205 of 300. Elapsed time 1.66 min\n",
      "Processed 69878 (100.0%) in 1.02 sec. BPR loss is 1.46E-01. Sample per second: 68457\n",
      "SLIM_BPR_Recommender: Epoch 206 of 300. Elapsed time 1.67 min\n",
      "Processed 69878 (100.0%) in 0.48 sec. BPR loss is 1.43E-01. Sample per second: 146711\n",
      "SLIM_BPR_Recommender: Epoch 207 of 300. Elapsed time 1.68 min\n",
      "Processed 69878 (100.0%) in 0.94 sec. BPR loss is 1.46E-01. Sample per second: 73881\n",
      "SLIM_BPR_Recommender: Epoch 208 of 300. Elapsed time 1.68 min\n",
      "Processed 69878 (100.0%) in 1.43 sec. BPR loss is 1.47E-01. Sample per second: 48718\n",
      "SLIM_BPR_Recommender: Epoch 209 of 300. Elapsed time 1.69 min\n",
      "Processed 69878 (100.0%) in 0.89 sec. BPR loss is 1.48E-01. Sample per second: 78528\n",
      "SLIM_BPR_Recommender: Epoch 210 of 300. Elapsed time 1.70 min\n",
      "Processed 69878 (100.0%) in 1.36 sec. BPR loss is 1.49E-01. Sample per second: 51329\n",
      "SLIM_BPR_Recommender: Epoch 211 of 300. Elapsed time 1.71 min\n",
      "Processed 69878 (100.0%) in 0.82 sec. BPR loss is 1.51E-01. Sample per second: 85331\n",
      "SLIM_BPR_Recommender: Epoch 212 of 300. Elapsed time 1.72 min\n",
      "Processed 69878 (100.0%) in 1.27 sec. BPR loss is 1.48E-01. Sample per second: 54831\n",
      "SLIM_BPR_Recommender: Epoch 213 of 300. Elapsed time 1.72 min\n",
      "Processed 69878 (100.0%) in 0.73 sec. BPR loss is 1.50E-01. Sample per second: 94948\n",
      "SLIM_BPR_Recommender: Epoch 214 of 300. Elapsed time 1.73 min\n",
      "Processed 69878 (100.0%) in 1.20 sec. BPR loss is 1.52E-01. Sample per second: 58208\n",
      "SLIM_BPR_Recommender: Epoch 215 of 300. Elapsed time 1.74 min\n",
      "Processed 69878 (100.0%) in 0.66 sec. BPR loss is 1.53E-01. Sample per second: 106192\n",
      "SLIM_BPR_Recommender: Epoch 216 of 300. Elapsed time 1.75 min\n",
      "Processed 69878 (100.0%) in 1.27 sec. BPR loss is 1.54E-01. Sample per second: 55091\n",
      "SLIM_BPR_Recommender: Epoch 217 of 300. Elapsed time 1.76 min\n",
      "Processed 69878 (100.0%) in 0.92 sec. BPR loss is 1.53E-01. Sample per second: 75730\n",
      "SLIM_BPR_Recommender: Epoch 218 of 300. Elapsed time 1.77 min\n",
      "Processed 69878 (100.0%) in 1.43 sec. BPR loss is 1.56E-01. Sample per second: 48859\n",
      "SLIM_BPR_Recommender: Epoch 219 of 300. Elapsed time 1.78 min\n",
      "Processed 69878 (100.0%) in 0.93 sec. BPR loss is 1.55E-01. Sample per second: 74920\n",
      "SLIM_BPR_Recommender: Epoch 220 of 300. Elapsed time 1.78 min\n",
      "Processed 69878 (100.0%) in 1.41 sec. BPR loss is 1.54E-01. Sample per second: 49621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: Epoch 221 of 300. Elapsed time 1.79 min\n",
      "Processed 69878 (100.0%) in 0.88 sec. BPR loss is 1.57E-01. Sample per second: 79341\n",
      "SLIM_BPR_Recommender: Epoch 222 of 300. Elapsed time 1.80 min\n",
      "Processed 69878 (100.0%) in 1.35 sec. BPR loss is 1.58E-01. Sample per second: 51714\n",
      "SLIM_BPR_Recommender: Epoch 223 of 300. Elapsed time 1.81 min\n",
      "Processed 69878 (100.0%) in 0.82 sec. BPR loss is 1.57E-01. Sample per second: 85449\n",
      "SLIM_BPR_Recommender: Epoch 224 of 300. Elapsed time 1.81 min\n",
      "Processed 69878 (100.0%) in 1.28 sec. BPR loss is 1.58E-01. Sample per second: 54409\n",
      "SLIM_BPR_Recommender: Epoch 225 of 300. Elapsed time 1.82 min\n",
      "Processed 69878 (100.0%) in 0.74 sec. BPR loss is 1.59E-01. Sample per second: 94324\n",
      "SLIM_BPR_Recommender: Epoch 226 of 300. Elapsed time 1.83 min\n",
      "Processed 69878 (100.0%) in 1.20 sec. BPR loss is 1.59E-01. Sample per second: 58214\n",
      "SLIM_BPR_Recommender: Epoch 227 of 300. Elapsed time 1.84 min\n",
      "Processed 69878 (100.0%) in 0.66 sec. BPR loss is 1.59E-01. Sample per second: 105732\n",
      "SLIM_BPR_Recommender: Epoch 228 of 300. Elapsed time 1.85 min\n",
      "Processed 69878 (100.0%) in 1.13 sec. BPR loss is 1.63E-01. Sample per second: 61871\n",
      "SLIM_BPR_Recommender: Epoch 229 of 300. Elapsed time 1.85 min\n",
      "Processed 69878 (100.0%) in 0.60 sec. BPR loss is 1.59E-01. Sample per second: 115707\n",
      "SLIM_BPR_Recommender: Epoch 230 of 300. Elapsed time 1.86 min\n",
      "Processed 69878 (100.0%) in 1.06 sec. BPR loss is 1.63E-01. Sample per second: 65894\n",
      "SLIM_BPR_Recommender: Epoch 231 of 300. Elapsed time 1.87 min\n",
      "Processed 69878 (100.0%) in 0.58 sec. BPR loss is 1.61E-01. Sample per second: 120079\n",
      "SLIM_BPR_Recommender: Epoch 232 of 300. Elapsed time 1.88 min\n",
      "Processed 69878 (100.0%) in 1.08 sec. BPR loss is 1.64E-01. Sample per second: 64977\n",
      "SLIM_BPR_Recommender: Epoch 233 of 300. Elapsed time 1.89 min\n",
      "Processed 69878 (100.0%) in 0.59 sec. BPR loss is 1.63E-01. Sample per second: 117464\n",
      "SLIM_BPR_Recommender: Epoch 234 of 300. Elapsed time 1.89 min\n",
      "Processed 69878 (100.0%) in 1.08 sec. BPR loss is 1.67E-01. Sample per second: 64500\n",
      "SLIM_BPR_Recommender: Epoch 235 of 300. Elapsed time 1.90 min\n",
      "Processed 69878 (100.0%) in 0.54 sec. BPR loss is 1.64E-01. Sample per second: 128236\n",
      "SLIM_BPR_Recommender: Epoch 236 of 300. Elapsed time 1.91 min\n",
      "Processed 69878 (100.0%) in 1.01 sec. BPR loss is 1.65E-01. Sample per second: 69019\n",
      "SLIM_BPR_Recommender: Epoch 237 of 300. Elapsed time 1.92 min\n",
      "Processed 69878 (100.0%) in 0.48 sec. BPR loss is 1.67E-01. Sample per second: 145289\n",
      "SLIM_BPR_Recommender: Epoch 238 of 300. Elapsed time 1.93 min\n",
      "Processed 69878 (100.0%) in 0.94 sec. BPR loss is 1.69E-01. Sample per second: 74457\n",
      "SLIM_BPR_Recommender: Epoch 239 of 300. Elapsed time 1.93 min\n",
      "Processed 69878 (100.0%) in 1.39 sec. BPR loss is 1.68E-01. Sample per second: 50126\n",
      "SLIM_BPR_Recommender: Epoch 240 of 300. Elapsed time 1.94 min\n",
      "Processed 69878 (100.0%) in 0.85 sec. BPR loss is 1.68E-01. Sample per second: 81771\n",
      "SLIM_BPR_Recommender: Epoch 241 of 300. Elapsed time 1.95 min\n",
      "Processed 69878 (100.0%) in 1.34 sec. BPR loss is 1.69E-01. Sample per second: 52262\n",
      "SLIM_BPR_Recommender: Epoch 242 of 300. Elapsed time 1.96 min\n",
      "Processed 69878 (100.0%) in 0.79 sec. BPR loss is 1.68E-01. Sample per second: 87940\n",
      "SLIM_BPR_Recommender: Epoch 243 of 300. Elapsed time 1.96 min\n",
      "Processed 69878 (100.0%) in 1.25 sec. BPR loss is 1.68E-01. Sample per second: 55941\n",
      "SLIM_BPR_Recommender: Epoch 244 of 300. Elapsed time 1.97 min\n",
      "Processed 69878 (100.0%) in 0.71 sec. BPR loss is 1.70E-01. Sample per second: 98745\n",
      "SLIM_BPR_Recommender: Epoch 245 of 300. Elapsed time 1.98 min\n",
      "Processed 69878 (100.0%) in 1.17 sec. BPR loss is 1.73E-01. Sample per second: 59970\n",
      "SLIM_BPR_Recommender: Epoch 246 of 300. Elapsed time 1.99 min\n",
      "Processed 69878 (100.0%) in 0.62 sec. BPR loss is 1.74E-01. Sample per second: 111854\n",
      "SLIM_BPR_Recommender: Epoch 247 of 300. Elapsed time 2.00 min\n",
      "Processed 69878 (100.0%) in 1.09 sec. BPR loss is 1.73E-01. Sample per second: 64332\n",
      "SLIM_BPR_Recommender: Epoch 248 of 300. Elapsed time 2.00 min\n",
      "Processed 69878 (100.0%) in 0.56 sec. BPR loss is 1.73E-01. Sample per second: 125285\n",
      "SLIM_BPR_Recommender: Epoch 249 of 300. Elapsed time 2.01 min\n",
      "Processed 69878 (100.0%) in 1.02 sec. BPR loss is 1.73E-01. Sample per second: 68692\n",
      "SLIM_BPR_Recommender: Epoch 250 of 300. Elapsed time 2.02 min\n",
      "Processed 69878 (100.0%) in 0.49 sec. BPR loss is 1.71E-01. Sample per second: 143252\n",
      "SLIM_BPR_Recommender: Epoch 251 of 300. Elapsed time 2.03 min\n",
      "Processed 69878 (100.0%) in 0.95 sec. BPR loss is 1.75E-01. Sample per second: 73763\n",
      "SLIM_BPR_Recommender: Epoch 252 of 300. Elapsed time 2.03 min\n",
      "Processed 69878 (100.0%) in 1.41 sec. BPR loss is 1.73E-01. Sample per second: 49669\n",
      "SLIM_BPR_Recommender: Epoch 253 of 300. Elapsed time 2.04 min\n",
      "Processed 69878 (100.0%) in 0.86 sec. BPR loss is 1.75E-01. Sample per second: 80842\n",
      "SLIM_BPR_Recommender: Epoch 254 of 300. Elapsed time 2.05 min\n",
      "Processed 69878 (100.0%) in 1.34 sec. BPR loss is 1.79E-01. Sample per second: 52229\n",
      "SLIM_BPR_Recommender: Epoch 255 of 300. Elapsed time 2.06 min\n",
      "Processed 69878 (100.0%) in 0.80 sec. BPR loss is 1.74E-01. Sample per second: 87738\n",
      "SLIM_BPR_Recommender: Epoch 256 of 300. Elapsed time 2.06 min\n",
      "Processed 69878 (100.0%) in 1.26 sec. BPR loss is 1.79E-01. Sample per second: 55504\n",
      "SLIM_BPR_Recommender: Epoch 257 of 300. Elapsed time 2.07 min\n",
      "Processed 69878 (100.0%) in 0.71 sec. BPR loss is 1.78E-01. Sample per second: 97663\n",
      "SLIM_BPR_Recommender: Epoch 258 of 300. Elapsed time 2.08 min\n",
      "Processed 69878 (100.0%) in 1.18 sec. BPR loss is 1.82E-01. Sample per second: 59418\n",
      "SLIM_BPR_Recommender: Epoch 259 of 300. Elapsed time 2.09 min\n",
      "Processed 69878 (100.0%) in 0.63 sec. BPR loss is 1.78E-01. Sample per second: 110121\n",
      "SLIM_BPR_Recommender: Epoch 260 of 300. Elapsed time 2.10 min\n",
      "Processed 69878 (100.0%) in 1.12 sec. BPR loss is 1.80E-01. Sample per second: 62332\n",
      "SLIM_BPR_Recommender: Epoch 261 of 300. Elapsed time 2.10 min\n",
      "Processed 69878 (100.0%) in 0.60 sec. BPR loss is 1.84E-01. Sample per second: 115773\n",
      "SLIM_BPR_Recommender: Epoch 262 of 300. Elapsed time 2.11 min\n",
      "Processed 69878 (100.0%) in 1.10 sec. BPR loss is 1.81E-01. Sample per second: 63291\n",
      "SLIM_BPR_Recommender: Epoch 263 of 300. Elapsed time 2.12 min\n",
      "Processed 69878 (100.0%) in 0.56 sec. BPR loss is 1.80E-01. Sample per second: 124428\n",
      "SLIM_BPR_Recommender: Epoch 264 of 300. Elapsed time 2.13 min\n",
      "Processed 69878 (100.0%) in 1.02 sec. BPR loss is 1.82E-01. Sample per second: 68702\n",
      "SLIM_BPR_Recommender: Epoch 265 of 300. Elapsed time 2.13 min\n",
      "Processed 69878 (100.0%) in 0.47 sec. BPR loss is 1.84E-01. Sample per second: 148881\n",
      "SLIM_BPR_Recommender: Epoch 266 of 300. Elapsed time 2.14 min\n",
      "Processed 69878 (100.0%) in 0.95 sec. BPR loss is 1.84E-01. Sample per second: 73876\n",
      "SLIM_BPR_Recommender: Epoch 267 of 300. Elapsed time 2.15 min\n",
      "Processed 69878 (100.0%) in 1.41 sec. BPR loss is 1.83E-01. Sample per second: 49475\n",
      "SLIM_BPR_Recommender: Epoch 268 of 300. Elapsed time 2.16 min\n",
      "Processed 69878 (100.0%) in 0.87 sec. BPR loss is 1.84E-01. Sample per second: 79959\n",
      "SLIM_BPR_Recommender: Epoch 269 of 300. Elapsed time 2.17 min\n",
      "Processed 69878 (100.0%) in 1.34 sec. BPR loss is 1.86E-01. Sample per second: 51976\n",
      "SLIM_BPR_Recommender: Epoch 270 of 300. Elapsed time 2.17 min\n",
      "Processed 69878 (100.0%) in 0.80 sec. BPR loss is 1.88E-01. Sample per second: 87472\n",
      "SLIM_BPR_Recommender: Epoch 271 of 300. Elapsed time 2.18 min\n",
      "Processed 69878 (100.0%) in 1.34 sec. BPR loss is 1.86E-01. Sample per second: 52134\n",
      "SLIM_BPR_Recommender: Epoch 272 of 300. Elapsed time 2.19 min\n",
      "Processed 69878 (100.0%) in 0.81 sec. BPR loss is 1.89E-01. Sample per second: 86175\n",
      "SLIM_BPR_Recommender: Epoch 273 of 300. Elapsed time 2.20 min\n",
      "Processed 69878 (100.0%) in 1.27 sec. BPR loss is 1.87E-01. Sample per second: 55047\n",
      "SLIM_BPR_Recommender: Epoch 274 of 300. Elapsed time 2.21 min\n",
      "Processed 69878 (100.0%) in 0.75 sec. BPR loss is 1.88E-01. Sample per second: 93056\n",
      "SLIM_BPR_Recommender: Epoch 275 of 300. Elapsed time 2.21 min\n",
      "Processed 69878 (100.0%) in 1.22 sec. BPR loss is 1.90E-01. Sample per second: 57069\n",
      "SLIM_BPR_Recommender: Epoch 276 of 300. Elapsed time 2.22 min\n",
      "Processed 69878 (100.0%) in 0.71 sec. BPR loss is 1.88E-01. Sample per second: 97874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLIM_BPR_Recommender: Epoch 277 of 300. Elapsed time 2.23 min\n",
      "Processed 69878 (100.0%) in 1.17 sec. BPR loss is 1.91E-01. Sample per second: 59701\n",
      "SLIM_BPR_Recommender: Epoch 278 of 300. Elapsed time 2.24 min\n",
      "Processed 69878 (100.0%) in 0.63 sec. BPR loss is 1.91E-01. Sample per second: 111804\n",
      "SLIM_BPR_Recommender: Epoch 279 of 300. Elapsed time 2.25 min\n",
      "Processed 69878 (100.0%) in 1.10 sec. BPR loss is 1.92E-01. Sample per second: 63726\n",
      "SLIM_BPR_Recommender: Epoch 280 of 300. Elapsed time 2.25 min\n",
      "Processed 69878 (100.0%) in 0.56 sec. BPR loss is 1.91E-01. Sample per second: 124994\n",
      "SLIM_BPR_Recommender: Epoch 281 of 300. Elapsed time 2.26 min\n",
      "Processed 69878 (100.0%) in 1.01 sec. BPR loss is 1.93E-01. Sample per second: 68806\n",
      "SLIM_BPR_Recommender: Epoch 282 of 300. Elapsed time 2.27 min\n",
      "Processed 69878 (100.0%) in 0.48 sec. BPR loss is 1.95E-01. Sample per second: 145849\n",
      "SLIM_BPR_Recommender: Epoch 283 of 300. Elapsed time 2.28 min\n",
      "Processed 69878 (100.0%) in 0.94 sec. BPR loss is 1.94E-01. Sample per second: 74209\n",
      "SLIM_BPR_Recommender: Epoch 284 of 300. Elapsed time 2.28 min\n",
      "Processed 69878 (100.0%) in 1.41 sec. BPR loss is 1.97E-01. Sample per second: 49694\n",
      "SLIM_BPR_Recommender: Epoch 285 of 300. Elapsed time 2.29 min\n",
      "Processed 69878 (100.0%) in 0.87 sec. BPR loss is 1.95E-01. Sample per second: 80163\n",
      "SLIM_BPR_Recommender: Epoch 286 of 300. Elapsed time 2.30 min\n",
      "Processed 69878 (100.0%) in 1.35 sec. BPR loss is 1.99E-01. Sample per second: 51677\n",
      "SLIM_BPR_Recommender: Epoch 287 of 300. Elapsed time 2.31 min\n",
      "Processed 69878 (100.0%) in 0.81 sec. BPR loss is 1.99E-01. Sample per second: 85768\n",
      "SLIM_BPR_Recommender: Epoch 288 of 300. Elapsed time 2.31 min\n",
      "Processed 69878 (100.0%) in 1.28 sec. BPR loss is 1.97E-01. Sample per second: 54710\n",
      "SLIM_BPR_Recommender: Epoch 289 of 300. Elapsed time 2.32 min\n",
      "Processed 69878 (100.0%) in 0.75 sec. BPR loss is 1.97E-01. Sample per second: 92583\n",
      "SLIM_BPR_Recommender: Epoch 290 of 300. Elapsed time 2.33 min\n",
      "Processed 69878 (100.0%) in 1.21 sec. BPR loss is 1.98E-01. Sample per second: 57736\n",
      "SLIM_BPR_Recommender: Epoch 291 of 300. Elapsed time 2.34 min\n",
      "Processed 69878 (100.0%) in 0.67 sec. BPR loss is 1.96E-01. Sample per second: 104166\n",
      "SLIM_BPR_Recommender: Epoch 292 of 300. Elapsed time 2.35 min\n",
      "Processed 69878 (100.0%) in 1.14 sec. BPR loss is 2.01E-01. Sample per second: 61117\n",
      "SLIM_BPR_Recommender: Epoch 293 of 300. Elapsed time 2.35 min\n",
      "Processed 69878 (100.0%) in 0.61 sec. BPR loss is 2.02E-01. Sample per second: 115142\n",
      "SLIM_BPR_Recommender: Epoch 294 of 300. Elapsed time 2.36 min\n",
      "Processed 69878 (100.0%) in 1.07 sec. BPR loss is 2.03E-01. Sample per second: 65343\n",
      "SLIM_BPR_Recommender: Epoch 295 of 300. Elapsed time 2.37 min\n",
      "Processed 69878 (100.0%) in 0.53 sec. BPR loss is 2.04E-01. Sample per second: 132113\n",
      "SLIM_BPR_Recommender: Epoch 296 of 300. Elapsed time 2.38 min\n",
      "Processed 69878 (100.0%) in 0.98 sec. BPR loss is 2.00E-01. Sample per second: 71052\n",
      "SLIM_BPR_Recommender: Epoch 297 of 300. Elapsed time 2.38 min\n",
      "Processed 69878 (100.0%) in 1.56 sec. BPR loss is 2.06E-01. Sample per second: 44683\n",
      "SLIM_BPR_Recommender: Epoch 298 of 300. Elapsed time 2.39 min\n",
      "Processed 69878 (100.0%) in 1.03 sec. BPR loss is 2.04E-01. Sample per second: 68015\n",
      "SLIM_BPR_Recommender: Epoch 299 of 300. Elapsed time 2.40 min\n",
      "Processed 69878 (100.0%) in 0.49 sec. BPR loss is 2.05E-01. Sample per second: 142340\n",
      "SLIM_BPR_Recommender: Epoch 300 of 300. Elapsed time 2.41 min\n",
      "SLIM_BPR_Recommender: Terminating at epoch 300. Elapsed time 2.48 min\n",
      "Deallocating Cython objects\n"
     ]
    }
   ],
   "source": [
    "topPop = TopPop(URM_train)\n",
    "topPop.fit()\n",
    "\n",
    "p3alpha = P3alphaRecommender(URM_train)\n",
    "p3alpha.fit()\n",
    "\n",
    "slimbpr = SLIM_BPR_Cython(URM_train)\n",
    "slimbpr.fit()\n",
    "\n",
    "\n",
    "other_algorithms = {\n",
    "    \"TopPop\": topPop,\n",
    "    \"P3alpha\": p3alpha,\n",
    "    \"SLIM_BPR\": slimbpr,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:47<00:00, 21.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Label</th>\n",
       "      <th>TopPop</th>\n",
       "      <th>P3alpha</th>\n",
       "      <th>SLIM_BPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>12852.0</td>\n",
       "      <td>0.247975</td>\n",
       "      <td>1.474523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>15175.0</td>\n",
       "      <td>0.258862</td>\n",
       "      <td>1.447398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "      <td>18545.0</td>\n",
       "      <td>0.362272</td>\n",
       "      <td>1.659632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>12191.0</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>1.375875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>16215.0</td>\n",
       "      <td>0.264557</td>\n",
       "      <td>1.551579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096335</th>\n",
       "      <td>69877</td>\n",
       "      <td>1283</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096336</th>\n",
       "      <td>69877</td>\n",
       "      <td>1429</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096337</th>\n",
       "      <td>69877</td>\n",
       "      <td>137</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096338</th>\n",
       "      <td>69877</td>\n",
       "      <td>1079</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096339</th>\n",
       "      <td>69877</td>\n",
       "      <td>411</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID  Label   TopPop   P3alpha  SLIM_BPR\n",
       "0             0     94  False  12852.0  0.247975  1.474523\n",
       "1             0     11   True  15175.0  0.258862  1.447398\n",
       "2             0    175  False  18545.0  0.362272  1.659632\n",
       "3             0     91  False  12191.0  0.225694  1.375875\n",
       "4             0     85  False  16215.0  0.264557  1.551579\n",
       "...         ...    ...    ...      ...       ...       ...\n",
       "2096335   69877   1283  False      NaN       NaN       NaN\n",
       "2096336   69877   1429  False      NaN       NaN       NaN\n",
       "2096337   69877    137  False      NaN       NaN       NaN\n",
       "2096338   69877   1079  False      NaN       NaN       NaN\n",
       "2096339   69877    411  False      NaN       NaN       NaN\n",
       "\n",
       "[2096340 rows x 6 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.set_index('UserID')\n",
    "\n",
    "for user_id in tqdm(range(1000)):       \n",
    "    for rec_label, rec_instance in other_algorithms.items():\n",
    "        \n",
    "        item_list = training_dataframe.loc[user_id, \"ItemID\"].values.tolist()\n",
    "        \n",
    "        all_item_scores = rec_instance._compute_item_score([user_id], items_to_compute = item_list)\n",
    "\n",
    "        training_dataframe.loc[user_id, rec_label] = all_item_scores[0, item_list] \n",
    "\n",
    "training_dataframe = training_dataframe.reset_index()\n",
    "training_dataframe = training_dataframe.rename(columns = {\"index\": \"UserID\"})\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The popularity of the item, the user activity etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Label</th>\n",
       "      <th>TopPop</th>\n",
       "      <th>P3alpha</th>\n",
       "      <th>SLIM_BPR</th>\n",
       "      <th>item_popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>12852.0</td>\n",
       "      <td>0.247975</td>\n",
       "      <td>1.474523</td>\n",
       "      <td>12852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>15175.0</td>\n",
       "      <td>0.258862</td>\n",
       "      <td>1.447398</td>\n",
       "      <td>15175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "      <td>18545.0</td>\n",
       "      <td>0.362272</td>\n",
       "      <td>1.659632</td>\n",
       "      <td>18545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>12191.0</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>1.375875</td>\n",
       "      <td>12191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>16215.0</td>\n",
       "      <td>0.264557</td>\n",
       "      <td>1.551579</td>\n",
       "      <td>16215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096335</th>\n",
       "      <td>69877</td>\n",
       "      <td>1283</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096336</th>\n",
       "      <td>69877</td>\n",
       "      <td>1429</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096337</th>\n",
       "      <td>69877</td>\n",
       "      <td>137</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096338</th>\n",
       "      <td>69877</td>\n",
       "      <td>1079</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096339</th>\n",
       "      <td>69877</td>\n",
       "      <td>411</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID  Label   TopPop   P3alpha  SLIM_BPR  item_popularity\n",
       "0             0     94  False  12852.0  0.247975  1.474523            12852\n",
       "1             0     11   True  15175.0  0.258862  1.447398            15175\n",
       "2             0    175  False  18545.0  0.362272  1.659632            18545\n",
       "3             0     91  False  12191.0  0.225694  1.375875            12191\n",
       "4             0     85  False  16215.0  0.264557  1.551579            16215\n",
       "...         ...    ...    ...      ...       ...       ...              ...\n",
       "2096335   69877   1283  False      NaN       NaN       NaN             8640\n",
       "2096336   69877   1429  False      NaN       NaN       NaN             5472\n",
       "2096337   69877    137  False      NaN       NaN       NaN             9852\n",
       "2096338   69877   1079  False      NaN       NaN       NaN             4782\n",
       "2096339   69877    411  False      NaN       NaN       NaN            10772\n",
       "\n",
       "[2096340 rows x 7 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_popularity = np.ediff1d(sps.csc_matrix(URM_train).indptr)\n",
    "\n",
    "training_dataframe['item_popularity'] = item_popularity[training_dataframe[\"ItemID\"].values.astype(int)]\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or the profile length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Label</th>\n",
       "      <th>TopPop</th>\n",
       "      <th>P3alpha</th>\n",
       "      <th>SLIM_BPR</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>user_profile_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>12852.0</td>\n",
       "      <td>0.247975</td>\n",
       "      <td>1.474523</td>\n",
       "      <td>12852</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>15175.0</td>\n",
       "      <td>0.258862</td>\n",
       "      <td>1.447398</td>\n",
       "      <td>15175</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "      <td>18545.0</td>\n",
       "      <td>0.362272</td>\n",
       "      <td>1.659632</td>\n",
       "      <td>18545</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>12191.0</td>\n",
       "      <td>0.225694</td>\n",
       "      <td>1.375875</td>\n",
       "      <td>12191</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>16215.0</td>\n",
       "      <td>0.264557</td>\n",
       "      <td>1.551579</td>\n",
       "      <td>16215</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096335</th>\n",
       "      <td>69877</td>\n",
       "      <td>1283</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8640</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096336</th>\n",
       "      <td>69877</td>\n",
       "      <td>1429</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5472</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096337</th>\n",
       "      <td>69877</td>\n",
       "      <td>137</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9852</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096338</th>\n",
       "      <td>69877</td>\n",
       "      <td>1079</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4782</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096339</th>\n",
       "      <td>69877</td>\n",
       "      <td>411</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10772</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UserID ItemID  Label   TopPop   P3alpha  SLIM_BPR  item_popularity  \\\n",
       "0             0     94  False  12852.0  0.247975  1.474523            12852   \n",
       "1             0     11   True  15175.0  0.258862  1.447398            15175   \n",
       "2             0    175  False  18545.0  0.362272  1.659632            18545   \n",
       "3             0     91  False  12191.0  0.225694  1.375875            12191   \n",
       "4             0     85  False  16215.0  0.264557  1.551579            16215   \n",
       "...         ...    ...    ...      ...       ...       ...              ...   \n",
       "2096335   69877   1283  False      NaN       NaN       NaN             8640   \n",
       "2096336   69877   1429  False      NaN       NaN       NaN             5472   \n",
       "2096337   69877    137  False      NaN       NaN       NaN             9852   \n",
       "2096338   69877   1079  False      NaN       NaN       NaN             4782   \n",
       "2096339   69877    411  False      NaN       NaN       NaN            10772   \n",
       "\n",
       "         user_profile_len  \n",
       "0                      13  \n",
       "1                      13  \n",
       "2                      13  \n",
       "3                      13  \n",
       "4                      13  \n",
       "...                   ...  \n",
       "2096335                32  \n",
       "2096336                32  \n",
       "2096337                32  \n",
       "2096338                32  \n",
       "2096339                32  \n",
       "\n",
       "[2096340 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_popularity = np.ediff1d(sps.csr_matrix(URM_train).indptr)\n",
    "\n",
    "training_dataframe['user_profile_len'] = user_popularity[training_dataframe[\"UserID\"].values.astype(int)]\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same can be done with item features\n",
    "\n",
    "It is better in this case to first create a sparse matrix replicating the rows needed and ten transform it into a sparse dataframe\n",
    "\n",
    "**WARNING** dataframes are not sparse structures and this may cause the memory requirements to explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10676</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10677</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10678</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10679</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10680</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10681 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
       "0      0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0   \n",
       "4      1.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "10676  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "10677  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "10678  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "10679  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "10680  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        14   15   16   17   18   19  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  \n",
       "10676  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10677  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10678  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10679  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "10680  1.0  1.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[10681 rows x 20 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df = pd.DataFrame.sparse.from_spmatrix(ICM_genres)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Label</th>\n",
       "      <th>TopPop</th>\n",
       "      <th>P3alpha</th>\n",
       "      <th>SLIM_BPR</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>user_profile_len</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11472</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1577</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33681</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1577</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39053</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1577</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56423</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1577</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67765</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1577</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>26787</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>60873</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917</th>\n",
       "      <td>60529</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10024</th>\n",
       "      <td>11660</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10025</th>\n",
       "      <td>39761</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2096340 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserID  Label  TopPop  P3alpha  SLIM_BPR  item_popularity  \\\n",
       "0       11472  False     NaN      NaN       NaN             1577   \n",
       "0       33681  False     NaN      NaN       NaN             1577   \n",
       "0       39053  False     NaN      NaN       NaN             1577   \n",
       "0       56423  False     NaN      NaN       NaN             1577   \n",
       "0       67765   True     NaN      NaN       NaN             1577   \n",
       "...       ...    ...     ...      ...       ...              ...   \n",
       "9718    26787  False     NaN      NaN       NaN                9   \n",
       "9718    60873  False     NaN      NaN       NaN                9   \n",
       "9917    60529  False     NaN      NaN       NaN               12   \n",
       "10024   11660  False     NaN      NaN       NaN               10   \n",
       "10025   39761  False     NaN      NaN       NaN                8   \n",
       "\n",
       "       user_profile_len    0    1    2  ...   10   11   12   13   14   15  \\\n",
       "0                    15  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0                    25  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0                    19  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0                    11  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "0                    15  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...                 ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9718                 20  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "9718                 38  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "9917                 22  0.0  0.0  0.0  ...  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "10024                18  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "10025                21  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        16   17   18   19  \n",
       "0      0.0  0.0  0.0  0.0  \n",
       "0      0.0  0.0  0.0  0.0  \n",
       "0      0.0  0.0  0.0  0.0  \n",
       "0      0.0  0.0  0.0  0.0  \n",
       "0      0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  \n",
       "9718   0.0  0.0  0.0  0.0  \n",
       "9718   0.0  0.0  0.0  0.0  \n",
       "9917   0.0  0.0  0.0  0.0  \n",
       "10024  0.0  0.0  1.0  0.0  \n",
       "10025  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[2096340 rows x 27 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = training_dataframe.set_index('ItemID').join(features_df, how='inner')\n",
    "training_dataframe = training_dataframe.reset_index()\n",
    "training_dataframe = training_dataframe.rename(columns = {\"index\": \"ItemID\"})\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then you can train XGBoost to rerank those prediction using as lable whether they should be recommended or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe = training_dataframe.sort_values(\"UserID\").reset_index()\n",
    "training_dataframe.drop(columns = ['index'], inplace=True)\n",
    "training_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the ranker one first needs to specify the size of the groups, a group is the dimension you rank on, in this case each group corresponds to a user. Since we have generated a fixed number of candidates for each user (30) all groups have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30, 30, ..., 30, 30, 30], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = training_dataframe.groupby(\"UserID\").size().values\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 50\n",
    "learning_rate = 1e-1\n",
    "reg_alpha = 1e-1\n",
    "reg_lambda = 1e-1\n",
    "max_depth = 5\n",
    "max_leaves = 0\n",
    "grow_policy = \"depthwise\"\n",
    "objective = \"pairwise\"\n",
    "booster = \"gbtree\"\n",
    "use_user_profile = False\n",
    "random_seed = None\n",
    "\n",
    "XGB_model = XGBRanker(objective='rank:{}'.format(objective),\n",
    "                      n_estimators = int(n_estimators),\n",
    "                      random_state = random_seed,\n",
    "                      learning_rate = learning_rate,\n",
    "                      reg_alpha = reg_alpha,\n",
    "                      reg_lambda = reg_lambda,\n",
    "                      max_depth = int(max_depth),\n",
    "                      max_leaves = int(max_leaves),\n",
    "                      grow_policy = grow_policy,\n",
    "                      verbosity = 0, # 2 if self.verbose else 0,\n",
    "                      booster = booster,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', callbacks=None, colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=1, early_stopping_rounds=None,\n",
       "          enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "          gamma=0, gpu_id=-1, grow_policy='depthwise', importance_type=None,\n",
       "          interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
       "          max_cat_threshold=64, max_cat_to_onehot=4, max_delta_step=0,\n",
       "          max_depth=5, max_leaves=0, min_child_weight=1, missing=nan,\n",
       "          monotone_constraints='()', n_estimators=50, n_jobs=0,\n",
       "          num_parallel_tree=1, predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = training_dataframe[\"Label\"]\n",
    "X_train = training_dataframe.drop(columns=[\"Label\"])\n",
    "\n",
    "XGB_model.fit(X_train,\n",
    "          y_train,\n",
    "          group=groups,\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the model is trained we can use it to compute predictions. Each prediction will refer to a specific user-item pair, which we will ten need to rank as we do in any other recommender model.\n",
    "\n",
    "### Important: In order to use this model to predict the score of new datapoints (i.e., new recommendations) we have to repeat the same data processing steps but:\n",
    "- We do not need a train-label split, we can user all the data we have to compute the predictions and the features\n",
    "- The recommendation models used to generate the scores should be trained on all the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20504402, 0.6008499 , 0.08450495, ..., 0.9010154 , 0.97065383,\n",
       "       0.49457154], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can have a look to the feature importance to assess which are the most informative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Weight (Frequence)'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(XGB_model, importance_type='weight', title='Weight (Frequence)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note, here ItemID and UserID are provided as integers, meaning that XGBoost will use them as any integer number and may split user groups according to whether their ID is < or > of a certain value. This makes no sense of course because the IDs are not ordinal, they are categorical and the specific numerical value of an IDs has no relation with the semantics of the problem. \n",
    "\n",
    "How to address this? \n",
    "- Use one-hot-encoded values -> drawback, the number of columns becomes very large\n",
    "- Use the native \"Categorical\" data type -> drawback, it is still experimental and may not work very well\n",
    "- Use another representation of the IDs, such as target encoding -> drawback, some further processing is needed and no teaching material is provided on this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to perform hyperparameter tuning?\n",
    "The issue with this method is that you need a label which should be an item the user has not interacted with but that is a correct recommendation. In practice the idea is:\n",
    "- Split the data in the usual training-validation-test\n",
    "- Split the training data in two: one part you use to train the recommenders and another you use as the hidden Label to train XGBoost\n",
    "- Evaluate your predictions on the validation data as you did for any other recommender model. Use this to select the optimal hyperparameters.\n",
    "- Given the selected hyperparameters, train the recommender models on all the available data and use all the available data to compute the features used by XGBoost.\n",
    "\n",
    "Challenge: Since the label we use for training XGBoost is the split of a split, it may happen that the actual correct recommendations are very few. This will result in a problem that is very unbalanced towards zero and will make the training difficult and the evaluation noisy. To mitigate this you may use k-fold cross validation and define the valdation result of a certain hyperparameter configuration as the average obtained with k different training-label splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
